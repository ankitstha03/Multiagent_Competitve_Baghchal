{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompetitivePlay_MultiAgent_BaghChal_v4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsA1wmuHFmz2"
      },
      "source": [
        "from queue import deque\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Add, Activation\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TXK0k3cF2cQ",
        "outputId": "04047db9-d40a-4119-edc8-632ca85ca8d4"
      },
      "source": [
        "from os.path import join\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTwhOgUXJkMe"
      },
      "source": [
        "PROJ = \"My Drive/Project\" # This is a custom path.\n",
        "PROJECT_PATH = os.path.join(ROOT, PROJ)\n",
        "IMAGEPATH=join(PROJECT_PATH, \"images\")\n",
        "BOARDPATH=join(PROJECT_PATH, \"boardresultscomp\")\n",
        "MODELPATH=join(PROJECT_PATH, \"models\")\n",
        "COMPPATH=join(PROJECT_PATH, \"modelscomp\")\n",
        "ACCEPTPATH=join(PROJECT_PATH, \"modescomplate\")\n",
        "FILTERPATH=join(PROJECT_PATH, \"modescomp3\")\n",
        "HUMANPATH=join(PROJECT_PATH, \"modelscomphuman2\")\n",
        "RESULTPATH=join(PROJECT_PATH, \"results\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFYcmz_OF2Xz"
      },
      "source": [
        "action_space={'11':0,'12':1,'13':2,'14':3,'15':4,'21':5,'22':6,'23':7,'24':8,'25':9,'31':10,'32':11,'33':12,'34':13,'35':14,'41':15,'42':16,'43':17,'44':18,'45':19,'51':20,'52':21,'53':22,'54':23,'55':24,'1112':25,'1121':26,'1122':27,'1113':28,'1131':29,'1133':30,'1213':31,'1211':32,'1222':33,'1232':34,'1214':35,'1312':36,'1314':37,'1323':38,'1322':39,'1324':40,'1333':41,'1331':42,'1315':43,'1311':44,'1335':45,'1415':46,'1413':47,'1424':48,'1412':49,'1434':50,'1525':51,'1524':52,'1514':53,'1513':54,'1533':55,'1535':56,'2131':57,'2111':58,'2122':59,'2141':60,'2123':61,'2212':62,'2232':63,'2213':64,'2233':65,'2231':66,'2221':67,'2223':68,'2211':69,'2242':70,'2244':71,'2224':72,'2324':73,'2313':74,'2333':75,'2322':76,'2325':77,'2321':78,'2343':79,'2413':80,'2433':81,'2414':82,'2415':83,'2423':84,'2425':85,'2434':86,'2435':87,'2442':88,'2444':89,'2422':90,'2515':91,'2524':92,'2535':93,'2545':94,'2523':95,'3132':96,'3121':97,'3122':98,'3142':99,'3141':100,'3113':101,'3133':102,'3151':103,'3111':104,'3153':105,'3242':106,'3231':107,'3233':108,'3222':109,'3212':110,'3234':111,'3252':112,'3332':113,'3344':114,'3323':115,'3343':116,'3322':117,'3342':118,'3334':119,'3324':120,'3313':121,'3355':122,'3331':123,'3315':124,'3353':125,'3351':126,'3311':127,'3335':128,'3424':129,'3444':130,'3433':131,'3435':132,'3454':133,'3432':134,'3414':135,'3545':136,'3544':137,'3525':138,'3534':139,'3524':140,'3513':141,'3555':142,'3533':143,'3515':144,'3553':145,'4142':146,'4151':147,'4131':148,'4143':149,'4121':150,'4232':151,'4241':152,'4233':153,'4231':154,'4243':155,'4251':156,'4252':157,'4253':158,'4244':159,'4224':160,'4222':161,'4342':162,'4344':163,'4333':164,'4353':165,'4345':166,'4341':167,'4323':168,'4454':169,'4433':170,'4455':171,'4445':172,'4443':173,'4453':174,'4434':175,'4435':176,'4442':177,'4424':178,'4422':179,'4544':180,'4555':181,'4535':182,'4525':183,'4543':184,'5142':185,'5141':186,'5152':187,'5131':188,'5133':189,'5153':190,'5242':191,'5251':192,'5253':193,'5254':194,'5232':195,'5354':196,'5344':197,'5343':198,'5342':199,'5352':200,'5355':201,'5333':202,'5331':203,'5351':204,'5335':205,'5444':206,'5455':207,'5453':208,'5434':209,'5452':210,'5545':211,'5554':212,'5544':213,'5535':214,'5533':215,'5553':216}\n",
        "reversed_action_space={v:k for k,v in action_space.items()}\n",
        "action_list=list(action_space.keys())\n",
        "bagh_moves_dict={(1,1):{(1,3),(3,1),(3,3)},(1,2):{(3,2),(1,4)},(1,3):{(3,3),(3,1),(1,5),(1,1),(3,5)},(1,4):{(1,2),(3,4)},(1,5):{(1,3),(3,3),(3,5)},(2,1):{(4,1),(2,3)},(2,2):{(4,2),(4,4),(2,4)},(2,3):{(2,5),(2,1),(4,3)},(2,4):{(4,2),(4,4),(2,2)},(2,5):{(4,5),(2,3)},(3,1):{(1,3),(3,3),(5,1),(1,1),(5,3)},(3,2):{(1,2),(3,4),(5,2)},(3,3):{(1,3),(5,5),(3,1),(1,5),(5,3),(5,1),(1,1),(3,5)},(3,4):{(5,4),(3,2),(1,4)},(3,5):{(1,3),(5,5),(3,3),(1,5),(5,3)},(4,1):{(4,3),(2,1)},(4,2):{(4,4),(2,4),(2,2)},(4,3):{(4,5),(4,1),(2,3)},(4,4):{(4,2),(2,4),(2,2)},(4,5):{(2,5),(4,3)},(5,1):{(3,1),(3,3),(5,3)},(5,2):{(5,4),(3,2)},(5,3):{(5,5),(3,3),(3,1),(5,1),(3,5)},(5,4):{(3,4),(5,2)},(5,5):{(3,5),(3,3),(5,3)}}\n",
        "connected_points_dict={(1,1):{(1,2),(2,1),(2,2)},(1,2):{(1,3),(1,1),(2,2)},(1,3):{(1,2),(1,4),(2,3),(2,2),(2,4)},(1,4):{(1,5),(1,3),(2,4)},(1,5):{(2,5),(2,4),(1,4)},(2,1):{(3,1),(1,1),(2,2)},(2,2):{(1,2),(3,2),(1,3),(3,3),(3,1),(2,1),(2,3),(1,1)},(2,3):{(2,4),(1,3),(3,3),(2,2)},(2,4):{(1,3),(3,3),(1,4),(1,5),(2,3),(2,5),(3,4),(3,5)},(2,5):{(1,5),(2,4),(3,5)},(3,1):{(3,2),(2,1),(2,2),(4,2),(4,1)},(3,2):{(4,2),(3,1),(3,3),(2,2)},(3,3):{(3,2),(4,4),(2,3),(4,3),(2,2),(4,2),(3,4),(2,4)},(3,4):{(2,4),(4,4),(3,3),(3,5)},(3,5):{(4,5),(4,4),(2,5),(3,4),(2,4)},(4,1):{(4,2),(5,1),(3,1)},(4,2):{(3,2),(4,1),(3,3),(3,1),(4,3),(5,1),(5,2),(5,3)},(4,3):{(4,2),(4,4),(3,3),(5,3)},(4,4):{(5,4),(3,3),(5,5),(4,5),(4,3),(5,3),(3,4),(3,5)},(4,5):{(4,4),(5,5),(3,5)},(5,1):{(4,2),(4,1),(5,2)},(5,2):{(4,2),(5,1),(5,3)},(5,3):{(5,4),(4,4),(4,3),(4,2),(5,2)},(5,4):{(4,4),(5,5),(5,3)},(5,5):{(4,5),(5,4),(4,4)}}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2KXmxcPF2hu"
      },
      "source": [
        "'''\n",
        "FEN -> Similar to Forsyth–Edwards Notation in chess\n",
        "    Starting fen => \"B3B/5/5/5/B3B G 0\"\n",
        "        {\"B3B/5/5/5/B3B\"(similar to chess,\"B\" - Bagh, \"G\" - Goat)}\n",
        "        {\"G\" or \"B\" represents who has next move}\n",
        "        {\"0\" number of moves by goat}\n",
        "PGN -> Portable Game Notation like in chess\n",
        "        Move to move tracking notation\n",
        "        <Move number>. G(<old_position>)<new_position> (...B<old_position><new_position>)\n",
        "        Example : 1.G33 B1122 2.G44\n",
        "        [ Note: G<new_position> for unplaced piece ]\n",
        "'''\n",
        "\n",
        "\n",
        "def render_points(p): return (103*(p[1]-1), 103*(p[0]-1))  # for pillow image\n",
        "\n",
        "\n",
        "BOARD_IMG = Image.open(os.path.join(IMAGEPATH, 'board.png'), 'r')\n",
        "BAGH_SPRITE = Image.open(os.path.join(IMAGEPATH, 'bagh.png'), 'r')\n",
        "GOAT_SPRITE = Image.open(os.path.join(IMAGEPATH, 'goat.png'), 'r')\n",
        "\n",
        "\n",
        "class Board:\n",
        "\n",
        "    def __init__(self, description=\"\"):\n",
        "        self.reset()\n",
        "        if description.strip():\n",
        "            self.pgn_converter(description.strip())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.board[index[0] - 1][index[1] - 1]\n",
        "\n",
        "    def __setitem__(self, index, value):\n",
        "        self.board[index[0] - 1][index[1] - 1] = value\n",
        "\n",
        "    @property\n",
        "    def no_of_moves_made(self):\n",
        "        return self.no_of_goat_moves + self.no_of_bagh_moves\n",
        "\n",
        "    def _possible_goat_moves(self):\n",
        "        # the function is independent of whose turn it is to play, use at your own risk.\n",
        "        move_list = set()\n",
        "        if self.no_of_goat_moves < 20:\n",
        "            return {f'G{x1}{y1}' for x1 in range(1, 6) for y1 in range(1, 6) if\n",
        "                    (x1, y1) not in self.bagh_points.union(self.goat_points)}\n",
        "        else:\n",
        "            for x1, y1 in self.goat_points:\n",
        "                move_list.update({f'G{x1}{y1}{x2}{y2}' for x2, y2 in self[x1, y1].valid_moves()})\n",
        "            return move_list\n",
        "\n",
        "    def _possible_bagh_moves(self):\n",
        "        # the function is independent of whose turn it is to play, use at your own risk.\n",
        "        move_list = set()\n",
        "        for x1, y1 in self.bagh_points:\n",
        "            move_list.update({f'B{x1}{y1}{x2}{y2}' for x2, y2 in self[x1, y1].valid_non_special_moves()})\n",
        "            move_list.update({f'Bx{x1}{y1}{x2}{y2}' for x2, y2 in self[x1, y1].valid_bagh_moves()})\n",
        "        return move_list\n",
        "\n",
        "    def possible_moves(self):\n",
        "        if self.is_game_over():\n",
        "            return 0\n",
        "        if self.next_turn == \"G\":\n",
        "            return self._possible_goat_moves()\n",
        "        else:\n",
        "            return self._possible_bagh_moves()\n",
        "\n",
        "    def possible_moves_vector(self):\n",
        "        moves_vector = np.zeros(217)\n",
        "        if self.is_game_over():\n",
        "            return moves_vector\n",
        "        if self.next_turn == \"G\" and self.no_of_goat_moves < 20:\n",
        "            for x1 in range(1, 6):\n",
        "                for y1 in range(1, 6):\n",
        "                    if (x1, y1) not in self.bagh_points.union(self.goat_points):\n",
        "                        moves_vector[action_space[f'{x1}{y1}']] = 1\n",
        "        elif self.next_turn == \"G\" and self.no_of_goat_moves >= 20:\n",
        "            for x1, y1 in self.goat_points:\n",
        "                for x2, y2 in self[x1, y1].valid_moves():\n",
        "                    moves_vector[action_space[f'{x1}{y1}{x2}{y2}']] = 1\n",
        "        else:\n",
        "            for x1, y1 in self.bagh_points:\n",
        "                for x2, y2 in self[x1, y1].valid_moves():\n",
        "                    moves_vector[action_space[f'{x1}{y1}{x2}{y2}']] = 1\n",
        "        return moves_vector\n",
        "\n",
        "    def pgn_converter(self, pgn):\n",
        "        move_list = re.findall(\n",
        "            r'[0-9]+\\.\\s*([G][1-5]{2,4})\\s*([B][x]?[1-5]{4})?', pgn)\n",
        "        for moves in move_list:\n",
        "            for move in moves:\n",
        "                if move == \"\":\n",
        "                    break\n",
        "                self.move(move)\n",
        "\n",
        "    def fen_to_board(self, fen):\n",
        "        rows = fen.split(\" \")[0].split(\"/\")\n",
        "        for x in range(5):\n",
        "            counter = 1\n",
        "            for y in rows[x]:\n",
        "                if y == \"B\":\n",
        "                    Bagh(self, (x + 1, counter))\n",
        "                    counter += 1\n",
        "                elif y == \"G\":\n",
        "                    Bagh(self, (x + 1, counter))\n",
        "                    counter += 1\n",
        "                else:\n",
        "                    for _ in range(int(y)):\n",
        "                        counter += 1\n",
        "\n",
        "    @property\n",
        "    def baghs_trapped(self):\n",
        "        counter = 0\n",
        "        for bagh in self.bagh_points:\n",
        "            if not self[bagh[0], bagh[1]].valid_moves():\n",
        "                counter += 1\n",
        "        return counter\n",
        "\n",
        "    @property\n",
        "    def all_goats_trapped(self):\n",
        "        return self.next_turn == \"G\" and not self._possible_goat_moves()\n",
        "\n",
        "    def show_info(func):\n",
        "        def wrapper(self):\n",
        "            if self.no_of_moves_made:\n",
        "                print(f\"Last move: {self.moves[-1]}\")\n",
        "            print(\n",
        "                f\"Goats Placed: {self.goats_placed}, Goats Captured: {self.goats_captured}, Baghs Trapped: {self.baghs_trapped}\")\n",
        "            func(self)\n",
        "            if self.is_game_over():\n",
        "                print(\"Game over.\")\n",
        "                if self.winner():\n",
        "                    print(f\"Winner : {self.winner()}\")\n",
        "                else:\n",
        "                    print(f\"The game is a draw.\")\n",
        "                return\n",
        "            print(f\"{self.next_turn} to play.\")\n",
        "            print(f\"Possible moves:\", end=\"\")\n",
        "            for move in self.possible_moves():\n",
        "                print(f\" {move}\", end=\"\")\n",
        "            print()\n",
        "            print()\n",
        "        return wrapper\n",
        "\n",
        "    @show_info\n",
        "    def show_board(self):\n",
        "        rep1 = ''' ¦ ＼         ¦         ／ ¦ ＼         ¦         ／ ¦    \n",
        " ¦   ＼       ¦       ／   ¦   ＼       ¦       ／   ¦    \n",
        " ¦     ＼     ¦     ／     ¦     ＼     ¦     ／     ¦    \n",
        " ¦       ＼   ¦   ／       ¦       ＼   ¦   ／       ¦    \n",
        " ¦         ＼ ¦ ／         ¦         ＼ ¦ ／         ¦    '''\n",
        "        rep2 = ''' ¦         ／ ¦ ＼         ¦          ／¦ ＼         ¦    \n",
        " ¦       ／   ¦   ＼       ¦        ／  ¦   ＼       ¦    \n",
        " ¦     ／     ¦     ＼     ¦      ／    ¦     ＼     ¦    \n",
        " ¦   ／       ¦       ＼   ¦    ／      ¦       ＼   ¦    \n",
        " ¦ ／         ¦         ＼ ¦  ／        ¦         ＼ ¦    '''\n",
        "        print(\n",
        "            f\"[{self[1,1]}]11--------[{self[1,2]}]12--------[{self[1,3]}]13--------[{self[1,4]}]14--------[{self[1,5]}]15\")\n",
        "        print(rep1)\n",
        "        print(\n",
        "            f\"[{self[2,1]}]21--------[{self[2,2]}]22--------[{self[2,3]}]23--------[{self[2,4]}]24--------[{self[2,5]}]25\")\n",
        "        print(rep2)\n",
        "        print(\n",
        "            f\"[{self[3,1]}]31--------[{self[3,2]}]32--------[{self[3,3]}]33--------[{self[3,4]}]34--------[{self[3,5]}]35\")\n",
        "        print(rep1)\n",
        "        print(\n",
        "            f\"[{self[4,1]}]41--------[{self[4,2]}]42--------[{self[4,3]}]43--------[{self[4,4]}]44--------[{self[4,5]}]45\")\n",
        "        print(rep2)\n",
        "        print(\n",
        "            f\"[{self[5,1]}]51--------[{self[5,2]}]52--------[{self[5,3]}]53--------[{self[5,4]}]54--------[{self[5,5]}]55\")\n",
        "\n",
        "    def validate_placement(self, move):\n",
        "        x1, y1 = int(move[1]), int(move[2])\n",
        "        self.validate_points(move, x1, y1)\n",
        "        if not self.goats_placed < 20:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} More than 20 goats cannot be placed\")\n",
        "        if self[x1, y1]:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} The coordinate is already occupied.\")\n",
        "        return True\n",
        "\n",
        "    def validate(self, move):\n",
        "        if self.is_game_over():\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} The game is already over.\")\n",
        "        move = move.strip()\n",
        "        if len(move) not in {3, 5, 6}:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Error ! Could not recognise the move.\")\n",
        "        if move[0] != self.next_turn:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Illegal Move.It is other side's turn.\")\n",
        "        if move[:2] == \"Bx\":\n",
        "            return self.validate_capture(move)\n",
        "        if len(move) == 3:\n",
        "            if self.goats_placed >= 20:\n",
        "                raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Futher piece cannot be placed.\")\n",
        "            if move[0] == \"B\":\n",
        "                raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Further Bagh cannot be placed.\")\n",
        "            return self.validate_placement(move)\n",
        "        if move[0] == \"G\" and len(move) == 5 and self.no_of_goat_moves < 20:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} All the goats must be placed first.\")\n",
        "        if move[:2] == \"Gx\":\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Goats cannot capture.\")\n",
        "        x1, y1, x2, y2 = int(move[1]), int(move[2]), int(move[3]), int(move[4])\n",
        "        self.validate_points(move, x1, y1, x2, y2)\n",
        "        self.validate_pp(move, x1, y1, move[0])\n",
        "\n",
        "        if move[0] == \"G\":\n",
        "            if not ((x2, y2) in self[x1, y1].valid_moves()):\n",
        "                raise Exception(\n",
        "                    f\"{(self.no_of_moves_made+2)//2}.{move} is not a valid move.\")\n",
        "        elif move[0] == \"B\":\n",
        "            if not ((x2, y2) in self[x1, y1].valid_non_special_moves()):\n",
        "                raise Exception(\n",
        "                    f\"{(self.no_of_moves_made+2)//2}.{move} is not a valid move.\")\n",
        "        return True\n",
        "\n",
        "    def validate_capture(self, move):\n",
        "        x1, y1, x2, y2 = int(move[2]), int(move[3]), int(move[4]), int(move[5])\n",
        "        self.validate_points(move, x1, y1, x2, y2)\n",
        "        self.validate_pp(move, x1, y1, move[0])\n",
        "        if not ((x2, y2) in self[x1, y1].valid_bagh_moves()):\n",
        "            raise Exception(\n",
        "                f\"{(self.no_of_moves_made+2)//2}.{move} is not a valid move.\")\n",
        "        return True\n",
        "\n",
        "    def validate_points(self, move, x1, y1, x2=1, y2=1):\n",
        "        if not (0 < x1 < 6 and 0 < y1 < 6 and 0 < x2 < 6 and 0 < y2 < 6):\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Invalid PGN. Coordinates not in range.\")\n",
        "\n",
        "    def validate_pp(self, move, x1, y1, p):\n",
        "        if not self[x1, y1]:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} ({x1},{y1}) is not occupied.\")\n",
        "        if self[x1, y1].__str__() != p:\n",
        "            raise Exception(f\"{(self.no_of_moves_made+2)//2}.{move} Piece at ({x1},{y1}) is other than specified.\")\n",
        "\n",
        "    def safe_move(self, move):\n",
        "        if len(move) == 3:\n",
        "            Goat(self, (int(move[1]), int(move[2])))\n",
        "            self.no_of_goat_moves += 1\n",
        "            self.goats_placed += 1\n",
        "        else:\n",
        "            if len(move) == 5:\n",
        "                x1, y1, x2, y2 = int(move[1]), int(\n",
        "                    move[2]), int(move[3]), int(move[4])\n",
        "            elif len(move) == 6:\n",
        "                x1, y1, x2, y2 = int(move[2]), int(\n",
        "                    move[3]), int(move[4]), int(move[5])\n",
        "                x3, y3 = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "                self[x3, y3] = 0\n",
        "                self.goat_points.remove((x3, y3))\n",
        "                self.goats_captured += 1\n",
        "            self[x1, y1] = 0\n",
        "            if move[0] == \"G\":\n",
        "                self.goat_points.remove((x1, y1))\n",
        "                Goat(self, (x2, y2))\n",
        "                self.no_of_goat_moves += 1\n",
        "            elif move[0] == \"B\":\n",
        "                self.bagh_points.remove((x1, y1))\n",
        "                Bagh(self, (x2, y2))\n",
        "                self.no_of_bagh_moves += 1\n",
        "\n",
        "        self.moves.append(move)\n",
        "        pgn_update = \"\"\n",
        "        if self.next_turn == \"G\":\n",
        "            pgn_update += f\"{self.no_of_goat_moves}. \"\n",
        "        pgn_update += move\n",
        "        self.pgn += \" \" + pgn_update\n",
        "        self.next_turn = \"G\" if self.next_turn == \"B\" else \"B\"\n",
        "\n",
        "        self.fen = self.board_to_fen()\n",
        "        self.fen_history.append(self.fen)\n",
        "        if self.no_of_goat_moves >= 20:\n",
        "            self.fen_count.update([self.fen.split(\" \")[0]])\n",
        "\n",
        "        if self.is_game_over():\n",
        "            if self.winner() == \"B\":\n",
        "                self.pgn += \"# 0-1\"\n",
        "            elif self.winner() == \"G\":\n",
        "                self.pgn += \"# 1-0\"\n",
        "            else:\n",
        "                self.pgn += \"# 1/2-1/2\"\n",
        "\n",
        "    def board_to_fen(self):\n",
        "        string = \"\"\n",
        "        for x in range(1, 6):\n",
        "            counter = 0\n",
        "            for y in range(1, 6):\n",
        "                if self[x, y]:\n",
        "                    counter = 0\n",
        "                    string += self[x, y].__str__()\n",
        "                else:\n",
        "                    counter += 1\n",
        "                    if y == 5 or self[x, y + 1] != 0:\n",
        "                        string += str(counter)\n",
        "            string += \"/\"\n",
        "        return f\"{string[:-1]} {self.next_turn} {self.no_of_goat_moves}\"\n",
        "\n",
        "    def move(self, move):\n",
        "        if self.validate(move):\n",
        "            self.safe_move(move)\n",
        "\n",
        "    def pure_move(self, move):\n",
        "        if len(move) == 2:\n",
        "            self.move(f\"G{move}\")\n",
        "        else:\n",
        "            x1, y1, x2, y2 = move\n",
        "            if (int(x1) - int(x2))**2 + (int(y1) - int(y2))**2 <= 2:\n",
        "                self.move(f\"{self.next_turn}{move}\")\n",
        "            else:\n",
        "                self.move(f'{self.next_turn}x{move}')\n",
        "\n",
        "    def is_game_over(self):\n",
        "        if self.goats_captured >= 5 or self.baghs_trapped == 4 or self.check_draw() or self.all_goats_trapped:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_draw(self):\n",
        "        if max(self.fen_count.values()) >= 3:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def winner(self):\n",
        "        if self.goats_captured >= 5 or self.all_goats_trapped:\n",
        "            return \"B\"\n",
        "        if self.baghs_trapped == 4:\n",
        "            return \"G\"\n",
        "        if self.check_draw():\n",
        "            return 0\n",
        "        raise Exception(\"Game is not over yet !\")\n",
        "\n",
        "    def fen_state(self, fen):\n",
        "        state = np.zeros((2, 5, 5))\n",
        "        rows = fen.split(\" \")[0].split(\"/\")\n",
        "        for x in range(5):\n",
        "            counter = 0\n",
        "            for y in rows[x]:\n",
        "                if y == \"G\":\n",
        "                    state[0, x, counter] = 1\n",
        "                    counter += 1\n",
        "                elif y == \"B\":\n",
        "                    state[1, x, counter] = 1\n",
        "                    counter += 1\n",
        "                else:\n",
        "                    for _ in range(int(y)):\n",
        "                        counter += 1\n",
        "        return state\n",
        "\n",
        "    def board_repr(self):\n",
        "        state = np.zeros((5, 5, 5))\n",
        "        state[[0, 1]] = self.fen_state(self.fen)\n",
        "        state[2, :, :] = self.goats_captured\n",
        "        state[3, :, :] = self.baghs_trapped\n",
        "        if self.next_turn == \"G\":\n",
        "            state[4, :, :] = 1\n",
        "        return state\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [[0, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 0]]\n",
        "        self.next_turn = \"G\"\n",
        "        self.no_of_goat_moves = 0\n",
        "        self.no_of_bagh_moves = 0\n",
        "        self.goats_placed = 0\n",
        "        self.goats_captured = 0\n",
        "        self.goat_points = set()\n",
        "        self.bagh_points = set()\n",
        "        self.fen = \"B3B/5/5/5/B3B G 0\"\n",
        "        self.fen_history = [self.fen]\n",
        "        self.fen_count = Counter([self.fen.split(\" \")[0]])\n",
        "        self.pgn = \"\"\n",
        "        self.moves = list()\n",
        "        self.fen_to_board(self.fen)\n",
        "\n",
        "    def recent_player(self):\n",
        "        return \"G\" if self.no_of_moves_made % 2 else \"B\"\n",
        "\n",
        "    def undo(self, no_of_moves=1):\n",
        "        if no_of_moves > self.no_of_moves_made:\n",
        "            raise Exception(\n",
        "                \"The number of moves to undo is greater than the number of moves made in the board.\")\n",
        "        move_list = self.moves\n",
        "        n = self.no_of_moves_made - no_of_moves\n",
        "        self.reset()\n",
        "        for move in move_list:\n",
        "            if move == \"\" or n == 0:\n",
        "                return move_list[-no_of_moves:]\n",
        "            self.move(move)\n",
        "            n -= 1\n",
        "\n",
        "    @show_info\n",
        "    def lightweight_show_board(self):\n",
        "        print(\"-\" * 26)\n",
        "        for row in self.board:\n",
        "            for x in row:\n",
        "                if x:\n",
        "                    print(f\"| {x.__str__()} \", end=\" \")\n",
        "                else:\n",
        "                    print(\"|   \", end=\" \")\n",
        "            print(\"|\")\n",
        "            print(\"-\" * 26)\n",
        "\n",
        "    def render(self):\n",
        "        img = Image.new(\"RGBA\", (480, 480), (255, 255, 255, 0))\n",
        "        img.paste(BOARD_IMG, (0, 0))\n",
        "        for point in self.goat_points:\n",
        "            img.paste(GOAT_SPRITE, render_points(point), mask=GOAT_SPRITE)\n",
        "        for point in self.bagh_points:\n",
        "            img.paste(BAGH_SPRITE, render_points(point), mask=BAGH_SPRITE)\n",
        "        display(img)\n",
        "        now = datetime.now()\n",
        "        filename='board'+ str(now) +'.png'\n",
        "        img.save(join(BOARDPATH, filename))\n",
        "\n",
        "\n",
        "class Piece:\n",
        "\n",
        "    def __init__(self, board, position=0):\n",
        "        if position:\n",
        "            if not (1, 1) <= position <= (5, 5):\n",
        "                raise Exception(f\"Invalid Coordinate for {self.__repr__()} - {position}\")\n",
        "            if board[position[0], position[1]]:\n",
        "                raise Exception(\n",
        "                    f\"Cannot place {self.__repr__()} at coordinate {position} occupied by {board[position[0],position[1]].__repr__()}\")\n",
        "        self.position = position\n",
        "        self.board = board\n",
        "        self.board[position[0], position[1]] = self\n",
        "        self.x=131.2+(position[1]-1)*103+30\n",
        "        self.y=114.5+(position[0]-1)*103+30\n",
        "\n",
        "    def connected_points(self):\n",
        "        if self.position:\n",
        "            return connected_points_dict[self.position]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def valid_moves(self):\n",
        "        return {x for x in self.connected_points()\n",
        "                if not self.board[x[0], x[1]]}\n",
        "\n",
        "\n",
        "class Bagh(Piece):\n",
        "\n",
        "    def __init__(self, board, position):\n",
        "        super(Bagh, self).__init__(board, position)\n",
        "        self.board.bagh_points.add(position)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"B\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Bagh\"\n",
        "\n",
        "    def special_connected_points(self):\n",
        "        return bagh_moves_dict[self.position]\n",
        "\n",
        "    def valid_bagh_moves(self):\n",
        "        return {x for x in self.special_connected_points()\n",
        "                if (not self.board[x[0], x[1]]) and self.board[\n",
        "                    (x[0] + self.position[0]) // 2, (x[1] + self.position[1]) // 2].__class__ == Goat}\n",
        "\n",
        "    def valid_moves(self):\n",
        "        return super(Bagh, self).valid_moves().union(self.valid_bagh_moves())\n",
        "\n",
        "    def valid_non_special_moves(self):\n",
        "        return super(Bagh, self).valid_moves()\n",
        "\n",
        "\n",
        "class Goat(Piece):\n",
        "\n",
        "    def __init__(self, board, position):\n",
        "        super(Goat, self).__init__(board, position)\n",
        "        self.board.goat_points.add(position)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"G\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Goat\"\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_bneKrNF2tt"
      },
      "source": [
        "def softmax(x):\n",
        "    arr=np.exp(x)\n",
        "    return arr/sum(arr)\n",
        "\n",
        "\n",
        "def mask_illegal(policy_vector, possible_moves):\n",
        "    # returns new softmax\n",
        "    valid_policy_vector = policy_vector * possible_moves\n",
        "\n",
        "    # if all legal positions are masked, return equal prob for all possible moves\n",
        "    if not valid_policy_vector.any():\n",
        "        return possible_moves/np.sum(possible_moves)\n",
        "\n",
        "    return softmax(np.log(valid_policy_vector+1e-10))\n",
        "\n",
        "\n",
        "def symmetry_board_moves(inputs):\n",
        "    ''' input -> [(board_repr(5,5,5),pi_vector,value),......]\n",
        "    output - > [(board_repr(5,5,5),pi_vector,value),......]\n",
        "    Each input has 8 symmetries including the given input i.e. rotation(0,90,180,270)+flip_horizontal(rotation(0,90,180,270))\n",
        "    These 8 output for single input encapsulates all rotation and flip horizontally or vertically.\n",
        "    '''\n",
        "    a = np.empty((5, 5), dtype=\"<U2\")\n",
        "    for x in range(1, 6):\n",
        "        for y in range(1, 6):\n",
        "            a[x - 1, y - 1] = f'{x}{y}'\n",
        "    output = []\n",
        "    for board_repr, move_probs, value in inputs:\n",
        "        moves2 = []\n",
        "        probs = move_probs[move_probs != 0]\n",
        "        for c in range(217):\n",
        "            if move_probs[c]:\n",
        "                moves2.append(reversed_action_space[c])\n",
        "        for i in range(4):\n",
        "            moves_after = []\n",
        "            tracker = np.rot90(a, i)\n",
        "            for move2 in moves2:\n",
        "                if len(move2) == 2:\n",
        "                    l = np.where(tracker == move2)\n",
        "                    moves_after.append(f'{l[0][0]+1}{l[1][0]+1}')\n",
        "                if len(move2) == 4:\n",
        "                    l1 = np.where(tracker == move2[:2])\n",
        "                    l2 = np.where(tracker == move2[2:])\n",
        "                    moves_after.append(f'{l1[0][0]+1}{l1[1][0]+1}{l2[0][0]+1}{l2[1][0]+1}')\n",
        "            new_l = [action_space[i] for i in moves_after]\n",
        "            new_move_probs = np.zeros(217)\n",
        "            new_move_probs[new_l] = probs\n",
        "            output.append((np.array([np.rot90(board_repr[0], i), np.rot90(board_repr[1], i), *board_repr[2:]]),\n",
        "                           new_move_probs, value))\n",
        "        for i in range(4):\n",
        "            moves_after = []\n",
        "            tracker = np.fliplr(np.rot90(a, i))\n",
        "            for move2 in moves2:\n",
        "                if len(move2) == 2:\n",
        "                    l = np.where(tracker == move2)\n",
        "                    moves_after.append(f'{l[0][0]+1}{l[1][0]+1}')\n",
        "                if len(move2) == 4:\n",
        "                    l1 = np.where(tracker == move2[:2])\n",
        "                    l2 = np.where(tracker == move2[2:])\n",
        "                    moves_after.append(f'{l1[0][0]+1}{l1[1][0]+1}{l2[0][0]+1}{l2[1][0]+1}')\n",
        "            new_l = [action_space[i] for i in moves_after]\n",
        "            new_move_probs = np.zeros(217)\n",
        "            new_move_probs[new_l] = probs\n",
        "            output.append((np.array(\n",
        "                [np.fliplr(np.rot90(board_repr[0], i)), np.fliplr(np.rot90(board_repr[1], i)), *board_repr[2:]]),\n",
        "                           new_move_probs, value))\n",
        "    return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ylkTtNbF2uV"
      },
      "source": [
        "action_list=np.array(action_list)\n",
        "\n",
        "class PolicyValueNet:\n",
        "    def __init__(self, model_file=None):\n",
        "        self.config = ModelConfig()\n",
        "\n",
        "        if model_file:\n",
        "            self.model = load_model(model_file, compile=True)\n",
        "        else:\n",
        "            self.create()\n",
        "            self.loss_train_op()\n",
        "\n",
        "    def create(self):\n",
        "\n",
        "        in_x = x = Input((5, 5, 5))\n",
        "\n",
        "        # (batch, channels, height, width)\n",
        "        x = Conv2D(filters=self.config.cnn_first_filter_num, kernel_size=self.config.cnn_first_filter_size, padding=\"same\",\n",
        "                   data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(self.config.l2_reg),\n",
        "                   name=f\"input1_conv-{self.config.cnn_filter_num}-{self.config.cnn_first_filter_size}x{self.config.cnn_first_filter_size}\")(\n",
        "            x)\n",
        "        x = Conv2D(filters=self.config.cnn_filter_num, kernel_size=self.config.cnn_first_filter_size, padding=\"same\",\n",
        "                   data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(self.config.l2_reg),\n",
        "                   name=f\"input_conv-{self.config.cnn_filter_num}-{self.config.cnn_first_filter_size}x{self.config.cnn_first_filter_size}\")(\n",
        "            x)\n",
        "        x = BatchNormalization(axis=1, name=\"input_batchnorm\")(x)\n",
        "        x = Activation(\"relu\", name=\"input_relu\")(x)\n",
        "\n",
        "        for i in range(self.config.resnet_N):\n",
        "            x = self.build_residual_block(x, i + 1)\n",
        "\n",
        "        res_out = x\n",
        "\n",
        "        # for policy output\n",
        "        x = Conv2D(filters=4, kernel_size=1, data_format=\"channels_first\", use_bias=False,\n",
        "                   kernel_regularizer=l2(self.config.l2_reg), name=\"policy_conv-4-1x1\")(res_out)\n",
        "        x = BatchNormalization(axis=1, name=\"policy_batchnorm\")(x)\n",
        "        x = Activation(\"relu\", name=\"policy_relu\")(x)\n",
        "        x = Flatten(name=\"policy_flatten\")(x)\n",
        "        ''' For action policy network output, BaghChal has 217 moves possible at any time period\n",
        "         (For more: lookup_table.py ->action_space)'''\n",
        "        policy_out = Dense(217, kernel_regularizer=l2(\n",
        "            self.config.l2_reg), activation=\"softmax\", name=\"policy_out\")(x)\n",
        "\n",
        "        # for value output\n",
        "        x = Conv2D(filters=2, kernel_size=1, data_format=\"channels_first\", use_bias=False,\n",
        "                   kernel_regularizer=l2(self.config.l2_reg), name=\"value_conv-2-1x1\")(res_out)\n",
        "        x = BatchNormalization(axis=1, name=\"value_batchnorm\")(x)\n",
        "        x = Activation(\"relu\", name=\"value_relu\")(x)\n",
        "        x = Flatten(name=\"value_flatten\")(x)\n",
        "        x = Dense(self.config.value_dense_size, kernel_regularizer=l2(\n",
        "            self.config.l2_reg), activation=\"relu\", name=\"value_dense\")(x)\n",
        "        value_out = Dense(1, kernel_regularizer=l2(\n",
        "            self.config.l2_reg), activation=\"tanh\", name=\"value_out\")(x)\n",
        "\n",
        "        self.model = Model(\n",
        "            in_x, [policy_out, value_out], name=\"BaghChal_model\")\n",
        "\n",
        "    def build_residual_block(self, x, index):\n",
        "        in_x = x\n",
        "        res_name = \"resnet\" + str(index)\n",
        "        x = Conv2D(filters=self.config.cnn_filter_num, kernel_size=self.config.cnn_filter_size, padding=\"same\",\n",
        "                   data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(self.config.l2_reg),\n",
        "                   name=f\"{res_name}_conv1-{self.config.cnn_filter_num}-{self.config.cnn_filter_size}x{self.config.cnn_filter_size}\")(\n",
        "            x)\n",
        "        x = BatchNormalization(axis=1, name=f\"{res_name}_batchnorm1\")(x)\n",
        "        x = Activation(\"relu\", name=f\"{res_name}_relu1\")(x)\n",
        "        x = Conv2D(filters=self.config.cnn_filter_num, kernel_size=self.config.cnn_filter_size, padding=\"same\",\n",
        "                   data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(self.config.l2_reg),\n",
        "                   name=f\"{res_name}_conv2-{self.config.cnn_filter_num}-{self.config.cnn_filter_size}x{self.config.cnn_filter_size}\")(\n",
        "            x)\n",
        "        x = BatchNormalization(axis=1, name=f\"{res_name}_batchnorm2\")(x)\n",
        "        x = Add(name=f\"{res_name}_add\")([in_x, x])\n",
        "        x = Activation(\"relu\", name=f\"{res_name}_relu2\")(x)\n",
        "        return x\n",
        "\n",
        "    def policy_value(self, state_input):\n",
        "        state_input = np.array(state_input)  # _on_batch(state_input_union)\n",
        "        results = self.model.predict(state_input)\n",
        "        return results\n",
        "\n",
        "    def policy_value_fn(self, board4):\n",
        "        \"\"\"\n",
        "        input: board\n",
        "        output: a list of (action, probability) tuples for each available action and the score of the board state\n",
        "        \"\"\"\n",
        "        legal_positions = board4.possible_moves_vector()\n",
        "        act_probs_raw, value = self.policy_value(np.expand_dims(board4.board_repr(),0))\n",
        "        act_probs_raw = mask_illegal(act_probs_raw[0], legal_positions)\n",
        "        index_list=np.where(legal_positions == 1)[0]\n",
        "        actions=action_list[index_list]\n",
        "        act_probs=act_probs_raw[index_list]\n",
        "        return zip(actions,act_probs), value[0]\n",
        "\n",
        "    def loss_train_op(self):\n",
        "        \"\"\"\n",
        "        Three loss terms：\n",
        "        loss = (z - v)^2 + pi^T * log(p) + c||theta||^2\n",
        "        \"\"\"\n",
        "        # get the train op\n",
        "        opt = Adam()\n",
        "        losses = ['categorical_crossentropy', 'mean_squared_error']\n",
        "        self.model.compile(optimizer=opt, loss=losses)\n",
        "\n",
        "    def save_model(self, model_filename):\n",
        "        \"\"\" save model to file \"\"\"\n",
        "        self.model.save(model_filename)\n",
        "\n",
        "    def train(self,board_repr2,mtcs_prob,winner,epochs):\n",
        "        board_repr2 = np.array(board_repr2)\n",
        "        mtcs_prob = np.array(mtcs_prob)\n",
        "        winner = np.array(winner)\n",
        "        self.model.fit(board_repr2, [mtcs_prob, winner],epochs=epochs)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjo0czC1F2vG"
      },
      "source": [
        "class Game:\n",
        "    def __init__(self, board5=Board()):\n",
        "        self.board5 = board5         \n",
        "\n",
        "    def start_play(self, GoatPlayer, BaghPlayer, show=True):\n",
        "        self.board5=Board()\n",
        "        states=[]\n",
        "        mcts_probs=[]\n",
        "        value=[]\n",
        "        \"\"\"start a game between two players\"\"\"\n",
        "        if show:\n",
        "            self.board5.lightweight_show_board()\n",
        "            self.board5.render()\n",
        "        while True:\n",
        "            player_to_move = GoatPlayer if self.board5.next_turn == \"G\" else BaghPlayer\n",
        "            idle_player = BaghPlayer if player_to_move == GoatPlayer else GoatPlayer\n",
        "            states.append(self.board5.board_repr())\n",
        "            player_index = 1 if self.board5.next_turn == \"G\" else -1\n",
        "            value.append(player_index)\n",
        "            if player_to_move.__class__==HumanPlayer:\n",
        "                move3 = player_to_move.get_action(self.board5)\n",
        "                a=np.zeros(217)\n",
        "                if len(move3)==3:\n",
        "                    a[action_space[move3[1:]]]=1\n",
        "                else:\n",
        "                    a[action_space[move3[-4:]]]=1\n",
        "                mcts_probs.append(a)\n",
        "                self.board5.safe_move(move3)\n",
        "                idle_player.update_move(move3)\n",
        "            else:\n",
        "                move3, a = player_to_move.get_action(self.board5, temp=0.8, return_prob=1)\n",
        "                if idle_player.__class__!=HumanPlayer:\n",
        "                    idle_player.update_move(move3)\n",
        "                self.board5.pure_move(move3)\n",
        "                mcts_probs.append(a)\n",
        "            if show:\n",
        "                self.board5.lightweight_show_board()\n",
        "                self.board5.render()\n",
        "            end = self.board5.is_game_over()\n",
        "            if end:\n",
        "                value=np.array(value)\n",
        "                if self.board5.winner() == \"B\":\n",
        "                    value *= -1\n",
        "                elif self.board5.winner() == 0:\n",
        "                    value *= 0\n",
        "                player.reset_player()\n",
        "                return zip(states, mcts_probs, value)\n",
        "\n",
        "    def start_self_play(self, player, show=True, temp=1e-3,greedy_player=None,who_greedy=\"\"):\n",
        "        \"\"\" start a self-play game using a MCTS player, reuse the search tree,\n",
        "        and store the self-play data: (state, mcts_probs, z) for training\n",
        "        \"\"\"\n",
        "        self.board5=Board()\n",
        "        states, mcts_probs, value = [], [], []\n",
        "        while True:\n",
        "            if greedy_player:\n",
        "                if who_greedy==\"B\":\n",
        "                    player_to_move = player if self.board5.next_turn == \"G\" else greedy_player\n",
        "                elif who_greedy==\"G\":\n",
        "                    player_to_move = player if self.board5.next_turn == \"B\" else greedy_player\n",
        "                else:\n",
        "                    player_to_move=greedy_player\n",
        "            else:\n",
        "                player_to_move = player\n",
        "            if greedy_player:\n",
        "                idle_player = player if player_to_move == player else greedy_player\n",
        "            move3, move_probs = player_to_move.get_action(self.board5,\n",
        "                                                 temp=temp,\n",
        "                                                 return_prob=1)\n",
        "            if greedy_player:\n",
        "                idle_player.update_move(move3)\n",
        "            # store the data\n",
        "            states.append(self.board5.board_repr())\n",
        "            mcts_probs.append(move_probs)\n",
        "\n",
        "            # temporarily store 1 for goat move and -1 for bagh\n",
        "            # later can multiply,ie if goat is winner <value>*1\n",
        "            # if bagh wins <value>*-1 , if draw *0\n",
        "            player_index = 1 if self.board5.next_turn == \"G\" else -1\n",
        "            value.append(player_index)\n",
        "\n",
        "            # perform a move\n",
        "            self.board5.pure_move(move3)\n",
        "            if show:\n",
        "                self.board5.lightweight_show_board()\n",
        "                # self.board5.render()\n",
        "            end = self.board5.is_game_over()\n",
        "            if end:\n",
        "                value=np.array(value)\n",
        "                # winner from the perspective of the current player of each state\n",
        "                if self.board5.winner() == \"B\":\n",
        "                    value *= -1\n",
        "                elif self.board5.winner() == 0:\n",
        "                    value *= 0\n",
        "                # reset MCTS root node\n",
        "                player.reset_player()\n",
        "                w=self.board5.winner()\n",
        "                return w, zip(states, mcts_probs, value)\n",
        "\n",
        "    def play_games(self, GoatPlayer, BaghPlayer, show=True, temp=1e-3):\n",
        "        \"\"\" start a self-play game using a MCTS player, reuse the search tree,\n",
        "        and store the self-play data: (state, mcts_probs, z) for training\n",
        "        \"\"\"\n",
        "        self.board5=Board()\n",
        "        states, mcts_probs, value = [], [], []\n",
        "        while True:\n",
        "            player_to_move = GoatPlayer if self.board5.next_turn == \"G\" else BaghPlayer\n",
        "            idle_player = BaghPlayer if player_to_move == GoatPlayer else GoatPlayer\n",
        "\n",
        "            move3, move_probs = player_to_move.get_action(self.board5,\n",
        "                                                 temp=temp,\n",
        "                                                 return_prob=1)\n",
        "            \n",
        "            idle_player.update_move(move3)\n",
        "            # store the data\n",
        "            states.append(self.board5.board_repr())\n",
        "            mcts_probs.append(move_probs)\n",
        "\n",
        "            # temporarily store 1 for goat move and -1 for bagh\n",
        "            # later can multiply,ie if goat is winner <value>*1\n",
        "            # if bagh wins <value>*-1 , if draw *0\n",
        "            player_index = 1 if self.board5.next_turn == \"G\" else -1\n",
        "            value.append(player_index)\n",
        "\n",
        "            # perform a move\n",
        "            self.board5.pure_move(move3)\n",
        "            if show:\n",
        "                self.board5.lightweight_show_board()\n",
        "                # self.board5.render()\n",
        "            end = self.board5.is_game_over()\n",
        "            if end:\n",
        "                value=np.array(value)\n",
        "                # winner from the perspective of the current player of each state\n",
        "                if self.board5.winner() == \"B\":\n",
        "                    value *= -1\n",
        "                elif self.board5.winner() == 0:\n",
        "                    value *= 0\n",
        "                # reset MCTS root node\n",
        "                player_to_move.reset_player()\n",
        "                idle_player.reset_player()\n",
        "                w=self.board5.winner()\n",
        "                return w, zip(states, mcts_probs, value)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y20gUHzF2mp"
      },
      "source": [
        "class ModelConfig:\n",
        "    def __init__(self):\n",
        "        self.cnn_first_filter_num=128\n",
        "        self.cnn_filter_num = 64\n",
        "        self.cnn_first_filter_size = 3\n",
        "        self.cnn_filter_size = 2\n",
        "        self.resnet_N = 5\n",
        "        self.l2_reg = 1e-4\n",
        "        self.value_dense_size = 32\n",
        "\n",
        "class TrainConfig:\n",
        "    def __init__(self):\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 0.8  # the temperature param\n",
        "        self.n_playout = 10  # num of simulations for each move\n",
        "        self.c_puct = 0.001\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 1  # mini-batch size for training\n",
        "\n",
        "        self.play_batch_size = 5 # how many games to play\n",
        "        self.epochs = 20  # num of train_steps for each update\n",
        "        self.ld=0.8\n",
        "\n",
        "        #c_puct=0.0001 ld =0.8\n",
        "class TrainGreedyConfig:\n",
        "    def __init__(self):\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 0.8  # the temperature param\n",
        "        self.n_playout = 10  # num of simulations for each move\n",
        "        self.c_puct = 1\n",
        "        self.buffer_size = 20000\n",
        "        self.batch_size = 1  # mini-batch size for training\n",
        "\n",
        "        self.play_batch_size = 1 # how many games to play\n",
        "        self.epochs = 10  # num of train_steps for each update\n",
        "        self.ld=0.8\n",
        "\n",
        "        #c_puct=0.0001 ld =0.8\n",
        "\n",
        "class PlayConfig:\n",
        "    def __init__(self):\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 0.8  # the temperature param\n",
        "        self.n_playout = 10  # num of simulations for each move\n",
        "        self.c_puct = 5\n",
        "        self.buffer_size = 20000\n",
        "        self.batch_size = 1  # mini-batch size for training\n",
        "\n",
        "        self.play_batch_size = 1 # how many games to play\n",
        "        self.epochs = 10  # num of train_steps for each update\n",
        "        self.ld=0.8\n",
        "\n",
        "        #c_puct=0.0001 ld =0.8\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldC40pDnF2fC"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, parent, prior_prob):\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.prior_prob = prior_prob\n",
        "        self.W = 0\n",
        "        self.n_visits = 0\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        return self.W / self.n_visits if self.n_visits != 0 else 0\n",
        "\n",
        "    def get_value(self, cpuct):\n",
        "        self.U = cpuct * self.prior_prob * \\\n",
        "                 (self.n_visits ** 0.5 / (1 + self.parent.n_visits))\n",
        "        return self.Q + self.U\n",
        "\n",
        "    def expand(self, action_prob2):\n",
        "        for action2, prob2 in action_prob2:\n",
        "            if action2 not in self.children:\n",
        "                self.children[action2] = Node(self, prob2)\n",
        "\n",
        "    def update(self, value):\n",
        "        self.n_visits += 1\n",
        "        self.W += value\n",
        "\n",
        "    def update_ancestors(self, value):\n",
        "        if self.parent:\n",
        "            self.parent.update(-value)\n",
        "        self.update(value) \n",
        "\n",
        "    def select(self, cpuct, sensible_moves):\n",
        "        # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.children.items()]\n",
        "        # tempchildren = {}\n",
        "        # print (sensible_moves)\n",
        "        # for action11 in sensible_moves:\n",
        "        #     print(action11[1:]) \n",
        "        #     if action11[1:] in self.children:\n",
        "                # print(\"here\")\n",
        "                # print(self.children[action[1:]])\n",
        "                # tempchildren[action11[1:]]=self.children[action11[1:]]\n",
        "        # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in tempchildren.items()]\n",
        "        # return max(tempchildren.items(),\n",
        "        #            key=lambda act_node: act_node[1].get_value(cpuct))\n",
        "        return max(self.children.items(),\n",
        "                   key=lambda act_node: act_node[1].get_value(cpuct))\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.children == {}\n",
        "\n",
        "    def is_root(self):\n",
        "        return self.parent is None\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, policy_value_fn, cpuct, n_playout):\n",
        "        self.root = Node(None, 1)\n",
        "        self.policy = policy_value_fn\n",
        "        self.cpuct = cpuct\n",
        "        self.n_playout = n_playout\n",
        "\n",
        "    def playout(self, board2):\n",
        "        node2 = self.root\n",
        "        # print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "        # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.root.children.items()]\n",
        "        # print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "        # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.root.children.items()]\n",
        "        # print(\"here2\")\n",
        "        while True:\n",
        "            if node2.is_leaf():\n",
        "                break\n",
        "            # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.root.children.items()]\n",
        "            # print(\"here3\")\n",
        "            action, node2 = node2.select(self.cpuct, board2.possible_moves())\n",
        "            # print (action)\n",
        "            board2.pure_move(action)\n",
        "        end = board2.is_game_over()\n",
        "        if not end:\n",
        "            action_probs2, leaf_value = self.policy(board2)\n",
        "            node2.expand(action_probs2)\n",
        "        else:\n",
        "            winner = board2.winner()\n",
        "            if winner == 0:  # tie\n",
        "                leaf_value = 0.0\n",
        "            else:\n",
        "                leaf_value = (\n",
        "                    1 if winner == board2.next_turn else -1\n",
        "                )\n",
        "\n",
        "        '''-ve because from the parent node's perspective the ,\n",
        "     leaf node is -leaf_value as we use max while selecting the node.'''\n",
        "        node2.update_ancestors(-leaf_value)\n",
        "\n",
        "    def get_move_probs(self, board3, temp=0.8):\n",
        "\n",
        "        for _ in range(self.n_playout):\n",
        "            board_copy = copy.deepcopy(board3)\n",
        "            self.playout(board_copy)\n",
        "\n",
        "        act_visits = [(act, node.n_visits)\n",
        "                      for act, node in self.root.children.items()]\n",
        "        acts2, visits = zip(*act_visits)\n",
        "        # print(acts2,visits)\n",
        "        act_probs2 = np.array(visits) ** (1 / temp)\n",
        "        # print(act_probs2)\n",
        "        act_probs2 = act_probs2 / sum(act_probs2)\n",
        "        # print(act_probs2)\n",
        "\n",
        "        # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.root.children.items()]\n",
        "        return acts2, act_probs2\n",
        "\n",
        "    def update_with_move(self, move2):\n",
        "\n",
        "        if move2 in self.root.children:\n",
        "            temp=self.root.children[move2].children\n",
        "            prob=self.root.children[move2].prior_prob\n",
        "            self.root = Node(None, prob)\n",
        "            self.children={}\n",
        "            self.root.children=temp\n",
        "            # print(\"-------------------------------------------------------------\")\n",
        "            # [print(act, f\"{node.Q} {node.n_visits}\") for act, node in self.root.children.items()]\n",
        "            # print(\"-------------------------------------------------------------\")\n",
        "        else:\n",
        "            self.root = Node(None, 1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf3rrEmaF2w4"
      },
      "source": [
        "class HumanPlayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_action(self, board6):\n",
        "        while True:\n",
        "            move4 = input(\"Enter your move: \")\n",
        "            try:\n",
        "                board6.validate(move4)\n",
        "                return move4\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "class MCTSPlayer:\n",
        "\n",
        "    def __init__(self, policy_value_fn,\n",
        "                 cpuct=5, n_playout=2000, is_selfplay=0):\n",
        "        self.mcts = MCTS(policy_value_fn, cpuct, n_playout)\n",
        "        self.is_selfplay = is_selfplay\n",
        "\n",
        "    def reset_player(self):\n",
        "        self.mcts.update_with_move(-1)\n",
        "        \n",
        "    def update_move(self, move4):\n",
        "        self.mcts.update_with_move(move4)\n",
        "    \n",
        "    def get_action(self, board6, temp=1e-3, return_prob=0):\n",
        "        sensible_moves = board6.possible_moves()\n",
        "        # print(sensible_moves)\n",
        "        # the pi vector returned by MCTS as in the alphaGo Zero paper\n",
        "        move_probs = np.zeros(217)\n",
        "        if sensible_moves:\n",
        "            acts, probs = self.mcts.get_move_probs(board6, temp)\n",
        "            counter=0\n",
        "            for act in acts:\n",
        "                index=action_space[act]\n",
        "                move_probs[index]=probs[counter]\n",
        "                counter+=1\n",
        "\n",
        "            if self.is_selfplay:\n",
        "                # add Dirichlet Noise for exploration (needed for\n",
        "                # self-play training)\n",
        "                # print (acts, probs)\n",
        "                move4 = np.random.choice(acts,p=\n",
        "                0.75* probs + 0.25 *\n",
        "                  np.random.dirichlet(0.3 * np.ones(len(probs))))\n",
        "                # update the root node and reuse the search tree\n",
        "                # print(move4)\n",
        "                # print(\"here\")\n",
        "                self.mcts.update_with_move(move4)\n",
        "            else:\n",
        "                # with the default temp=1e-3, it is almost equivalent\n",
        "                # to choosing the move with the highest prob\n",
        "                # print (acts, probs)\n",
        "                move4 = np.random.choice(acts, p=probs)\n",
        "                # reset the root node\n",
        "                self.mcts.update_with_move(move4)\n",
        "\n",
        "            if return_prob:\n",
        "                return move4, move_probs\n",
        "            else:\n",
        "                return move4\n",
        "        else:\n",
        "            print(\"WARNING ! Game is over.\")\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYoCDV6VF2zX"
      },
      "source": [
        "#Self Play agents\n",
        "class TrainPipeline():\n",
        "    def __init__(self, init_model=None):\n",
        "\n",
        "        # params of the board and the game\n",
        "        self.game = Game()\n",
        "        # training params\n",
        "        self.config=TrainConfig()\n",
        "        self.greedy_config=TrainGreedyConfig()\n",
        "        self.play_config=PlayConfig()\n",
        "        self.data_buffer = deque(maxlen=self.config.buffer_size)\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net = PolicyValueNet()\n",
        "\n",
        "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "        self.mcts_player_greedy = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      cpuct=self.greedy_config.c_puct,\n",
        "                                      n_playout=self.greedy_config.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "\n",
        "    def collect_selfplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Self-Play Data\")\n",
        "        print(\"Self-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best.h5')):\n",
        "                self.policy_value_net=PolicyValueNet(os.path.join(MODELPATH, 'best.h5'))\n",
        "                self.mcts_player=MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "            winner, play_data = self.game.start_self_play(self.mcts_player, show=False,\n",
        "                                                          temp=self.config.temp)\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "        print(\"\\n Self-Play Data Collection Completed\")\n",
        "\n",
        "    def start_play_batch (self, GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=self.game.play_games(GoatPlayer, BaghPlayer, show=False, temp=self.play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "\n",
        "    def policy_update(self, i):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        state_batch = [data[0] for data in self.data_buffer]\n",
        "        mcts_probs_batch = [data[1] for data in self.data_buffer]\n",
        "        winner_batch = [data[2] for data in self.data_buffer]\n",
        "        self.policy_value_net.train(state_batch,mcts_probs_batch,winner_batch,self.config.epochs)\n",
        "        if os.path.exists(os.path.join(MODELPATH, 'best.h5')):\n",
        "            self.policy_value_net.save_model(os.path.join(MODELPATH, 'temp.h5'))\n",
        "            newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp.h5'))\n",
        "            newplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best.h5'))\n",
        "            oldplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            print(\"Pitting new model vs previous best model\")\n",
        "            print(\"New player as goat\")\n",
        "            nwing, drawg, pwing =self.start_play_batch(newplayer, oldplayer, n_games=10, show=False)\n",
        "            print([nwing, drawg, pwing])\n",
        "            print(\"New player as bagh\")\n",
        "            pwinb, drawb, nwinb =self.start_play_batch(oldplayer, newplayer, n_games=10, show=False)\n",
        "            print([nwinb, drawb, pwinb])\n",
        "            print(\"Pitting Completed\")\n",
        "            nwin=nwing+nwinb\n",
        "            pwin=pwing+pwinb\n",
        "            draw=drawg+drawb\n",
        "            print([nwin, draw, pwin])\n",
        "            if (nwin>=pwin+1):\n",
        "                print(\"New model accepted\")\n",
        "                self.policy_value_net.save_model(os.path.join(MODELPATH, 'best.h5'))\n",
        "                modelname='accepted_'+str(i)+'.h5'\n",
        "            else:\n",
        "                print(\"New model rejected\")\n",
        "                modelname='rejected_'+str(i)+'.h5'\n",
        "            self.policy_value_net.save_model(os.path.join(MODELPATH, modelname))\n",
        "        else:\n",
        "            self.policy_value_net.save_model(os.path.join(MODELPATH, 'best.h5'))\n",
        "            self.policy_value_net.save_model(os.path.join(MODELPATH, 'accepted_0.h5'))\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        for i in range(100):\n",
        "            try:\n",
        "                self.collect_selfplay_data(self.config.play_batch_size)\n",
        "                self.policy_update(i)\n",
        "            except KeyboardInterrupt:\n",
        "                print('\\n\\rquit')\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        self.policy_value_net.model.summary()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TrainPipeline().run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h29BpsqWjg-w"
      },
      "source": [
        "#Competitive agent with alternate training\n",
        "class TrainPipelineCompetitive():\n",
        "    def __init__(self, init_model=None):\n",
        "\n",
        "        # params of the board and the game\n",
        "        self.game = Game()\n",
        "        self.turn = \"G\"\n",
        "        # training params\n",
        "        self.config=TrainConfig()\n",
        "        self.greedy_config=TrainGreedyConfig()\n",
        "        self.play_config=PlayConfig()\n",
        "        self.data_buffer = deque(maxlen=self.config.buffer_size)\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net_goat = PolicyValueNet()\n",
        "            self.policy_value_net_bagh = PolicyValueNet()\n",
        "\n",
        "        self.mcts_player_goat = MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        self.mcts_player_bagh = MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        # self.mcts_player_greedy = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "        #                               cpuct=self.greedy_config.c_puct,\n",
        "        #                               n_playout=self.greedy_config.n_playout,\n",
        "        #                               is_selfplay=1)\n",
        "\n",
        "    def collect_compplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Comp-Play Data\")\n",
        "        print(\"Comp-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.mcts_player_goat=MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.mcts_player_bagh=MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            winner, play_data = self.game.play_games(self.mcts_player_goat, self.mcts_player_bagh, show=False, temp=self.config.temp)\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "        print(\"\\n Comp-Play Data Collection Completed\")\n",
        "\n",
        "    def start_play_batch_competitve (self, GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=self.game.play_games(GoatPlayer, BaghPlayer, show=False, temp=self.play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "\n",
        "    def policy_update_competitive(self, i):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        state_batch = [data[0] for data in self.data_buffer]\n",
        "        mcts_probs_batch = [data[1] for data in self.data_buffer]\n",
        "        winner_batch = [data[2] for data in self.data_buffer]\n",
        "        if self.turn==\"G\":\n",
        "            self.policy_value_net_goat.train(state_batch,mcts_probs_batch,winner_batch,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newgoatplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                oldbaghplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new goat model vs previous bagh model\")\n",
        "                nwin, draw, pwin =self.start_play_batch_competitve(newgoatplayer, oldbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin>=pwin+1):\n",
        "                    print(\"New goat model accepted\")\n",
        "                    self.turn=\"B\"\n",
        "                    self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                    modelname='acceptedgoat_'+str(i)+'.h5'\n",
        "                else:\n",
        "                    print(\"New goat model rejected\")\n",
        "                    modelname='rejectedgoat_'+str(i)+'.h5'\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"B\"\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'acceptedgoat_0.h5'))\n",
        "        else:\n",
        "            winner_batch2 = [data*-1 for data in winner_batch]\n",
        "            self.policy_value_net_bagh.train(state_batch,mcts_probs_batch,winner_batch2,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newbaghplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                oldgoatplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new bagh model vs previous goat model\")\n",
        "                pwin, draw, nwin =self.start_play_batch_competitve(oldgoatplayer, newbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin>=pwin+1):\n",
        "                    print(\"New bagh model accepted\")\n",
        "                    self.turn=\"G\"\n",
        "                    self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                    modelname='acceptedbagh_'+str(i)+'.h5'\n",
        "                else:\n",
        "                    print(\"New bagh model rejected\")\n",
        "                    modelname='rejectedbagh_'+str(i)+'.h5'\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"G\"\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'acceptedbagh_0.h5'))\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        for i in range(200):\n",
        "            try:\n",
        "                self.collect_compplay_data(self.config.play_batch_size)\n",
        "                self.policy_update_competitive(i)\n",
        "            except KeyboardInterrupt:\n",
        "                print('\\n\\rquit')\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        self.policy_value_net_goat.model.summary()\n",
        "        self.policy_value_net_bagh.model.summary()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TrainPipelineCompetitive().run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THCGWBowB5xV"
      },
      "source": [
        "#Competitive agents with data filter\n",
        "class TrainPipelineCompetitive():\n",
        "    def __init__(self, init_model=None):\n",
        "\n",
        "        # params of the board and the game\n",
        "        self.game = Game()\n",
        "        self.turn = \"G\"\n",
        "        # training params\n",
        "        self.config=TrainConfig()\n",
        "        self.greedy_config=TrainGreedyConfig()\n",
        "        self.play_config=PlayConfig()\n",
        "        self.data_buffer_goat = deque(maxlen=self.config.buffer_size)\n",
        "        self.data_buffer_bagh = deque(maxlen=self.config.buffer_size)\n",
        "        self.oldgoat=-20\n",
        "        self.oldbagh=-20\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net_goat = PolicyValueNet()\n",
        "            self.policy_value_net_bagh = PolicyValueNet()\n",
        "\n",
        "        self.mcts_player_goat = MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        self.mcts_player_bagh = MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        # self.mcts_player_greedy = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "        #                               cpuct=self.greedy_config.c_puct,\n",
        "        #                               n_playout=self.greedy_config.n_playout,\n",
        "        #                               is_selfplay=1)\n",
        "\n",
        "    def collect_compplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Comp-Play Data\")\n",
        "        print(\"Comp-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.mcts_player_goat=MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.mcts_player_bagh=MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            winner, play_data = self.game.play_games(self.mcts_player_goat, self.mcts_player_bagh, show=False, temp=self.config.temp)\n",
        "            print(winner, end=\" \")\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            if winner==\"G\":\n",
        "                for data in play_data:\n",
        "                    if data[2]==1:\n",
        "                        self.data_buffer_goat.append(data)\n",
        "                    else:\n",
        "                        self.data_buffer_bagh.append(data)\n",
        "            if winner==\"B\":\n",
        "                  for data in play_data:\n",
        "                    if data[2]==1:\n",
        "                        self.data_buffer_bagh.append(data)\n",
        "                    else:\n",
        "                        self.data_buffer_goat.append(data)\n",
        "            if winner==0:\n",
        "                self.data_buffer_goat.extend(play_data)\n",
        "                self.data_buffer_bagh.extend(play_data)\n",
        "        print(\"\\n Comp-Play Data Collection Completed\")\n",
        "\n",
        "    def start_play_batch_competitve (self, GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=self.game.play_games(GoatPlayer, BaghPlayer, show=False, temp=self.play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "\n",
        "    def policy_update_competitive(self, i):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        state_batch_goat = [data[0] for data in self.data_buffer_goat]\n",
        "        mcts_probs_batch_goat = [data[1] for data in self.data_buffer_goat]\n",
        "        winner_batch_goat = [data[2] for data in self.data_buffer_goat]\n",
        "        print(winner_batch_goat)\n",
        "        state_batch_bagh = [data[0] for data in self.data_buffer_bagh]\n",
        "        mcts_probs_batch_bagh = [data[1] for data in self.data_buffer_bagh]\n",
        "        winner_batch_bagh = [data[2] for data in self.data_buffer_bagh]\n",
        "        print(winner_batch_bagh)\n",
        "        if self.turn==\"G\":\n",
        "            self.policy_value_net_goat.train(state_batch_goat,mcts_probs_batch_goat,winner_batch_goat,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newgoatplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                oldbaghplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new goat model vs previous bagh model\")\n",
        "                nwin, draw, pwin =self.start_play_batch_competitve(newgoatplayer, oldbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin-pwin>self.oldgoat):\n",
        "                    print(\"New goat model accepted\")\n",
        "                    self.turn=\"B\"\n",
        "                    self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                    modelname='acceptedgoat_'+str(i)+'.h5'\n",
        "                    self.oldbagh=pwin-nwin\n",
        "                else:\n",
        "                    print(\"New goat model rejected\")\n",
        "                    modelname='rejectedgoat_'+str(i)+'.h5'\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"B\"\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'acceptedgoat_0.h5'))\n",
        "        else:\n",
        "            # winner_batch2 = [data*-1 for data in winner_batch]\n",
        "            self.policy_value_net_bagh.train(state_batch_bagh,mcts_probs_batch_bagh,winner_batch_bagh,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newbaghplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                oldgoatplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new bagh model vs previous goat model\")\n",
        "                pwin, draw, nwin =self.start_play_batch_competitve(oldgoatplayer, newbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin-pwin>self.oldbagh):\n",
        "                    print(\"New bagh model accepted\")\n",
        "                    self.turn=\"G\"\n",
        "                    self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                    modelname='acceptedbagh_'+str(i)+'.h5'\n",
        "                    self.oldgoat=pwin-nwin\n",
        "                else:\n",
        "                    print(\"New bagh model rejected\")\n",
        "                    modelname='rejectedbagh_'+str(i)+'.h5'\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"G\"\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'acceptedbagh_0.h5'))\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        for i in range(150):\n",
        "            try:\n",
        "                self.collect_compplay_data(self.config.play_batch_size)\n",
        "                self.policy_update_competitive(i)\n",
        "            except KeyboardInterrupt:\n",
        "                print('\\n\\rquit')\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        self.policy_value_net_goat.model.summary()\n",
        "        self.policy_value_net_bagh.model.summary()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TrainPipelineCompetitive().run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTzZn-4G_374"
      },
      "source": [
        "#Competitive agent with accepted condition changed\n",
        "class TrainPipelineCompetitive():\n",
        "    def __init__(self, init_model=None):\n",
        "\n",
        "        # params of the board and the game\n",
        "        self.game = Game()\n",
        "        self.turn = \"G\"\n",
        "        # training params\n",
        "        self.config=TrainConfig()\n",
        "        self.greedy_config=TrainGreedyConfig()\n",
        "        self.play_config=PlayConfig()\n",
        "        self.data_buffer = deque(maxlen=self.config.buffer_size)\n",
        "        self.oldgoat=-20\n",
        "        self.oldbagh=-20\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net_goat = PolicyValueNet()\n",
        "            self.policy_value_net_bagh = PolicyValueNet()\n",
        "\n",
        "        self.mcts_player_goat = MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        self.mcts_player_bagh = MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        # self.mcts_player_greedy = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "        #                               cpuct=self.greedy_config.c_puct,\n",
        "        #                               n_playout=self.greedy_config.n_playout,\n",
        "        #                               is_selfplay=1)\n",
        "\n",
        "    def collect_compplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Comp-Play Data\")\n",
        "        print(\"Comp-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.mcts_player_goat=MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.mcts_player_bagh=MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            winner, play_data = self.game.play_games(self.mcts_player_goat, self.mcts_player_bagh, show=False, temp=self.config.temp)\n",
        "            print(winner, end=\" \")\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "        print(\"\\n Comp-Play Data Collection Completed\")\n",
        "\n",
        "    def start_play_batch_competitve (self, GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=self.game.play_games(GoatPlayer, BaghPlayer, show=False, temp=self.play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "\n",
        "    def policy_update_competitive(self, i):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        state_batch_goat = [data[0] for data in self.data_buffer]\n",
        "        mcts_probs_batch_goat = [data[1] for data in self.data_buffer]\n",
        "        winner_batch_goat = [data[2] for data in self.data_buffer]\n",
        "        print(winner_batch_goat)\n",
        "        state_batch_bagh = [data[0] for data in self.data_buffer]\n",
        "        mcts_probs_batch_bagh = [data[1] for data in self.data_buffer]\n",
        "        winner_batch_bagh = [data[2] for data in self.data_buffer]\n",
        "        print(winner_batch_bagh)\n",
        "        if self.turn==\"G\":\n",
        "            self.policy_value_net_goat.train(state_batch_goat,mcts_probs_batch_goat,winner_batch_goat,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "                newgoatplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                oldbaghplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new goat model vs previous bagh model\")\n",
        "                nwin, draw, pwin =self.start_play_batch_competitve(newgoatplayer, oldbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin-pwin>self.oldgoat):\n",
        "                    print(\"New goat model accepted\")\n",
        "                    self.turn=\"B\"\n",
        "                    self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                    modelname='acceptedgoat_'+str(i)+'.h5'\n",
        "                    self.oldbagh=pwin-nwin\n",
        "                else:\n",
        "                    print(\"New goat model rejected\")\n",
        "                    modelname='rejectedgoat_'+str(i)+'.h5'\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"B\"\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'acceptedgoat_0.h5'))\n",
        "        else:\n",
        "            # winner_batch2 = [data*-1 for data in winner_batch]\n",
        "            self.policy_value_net_bagh.train(state_batch_bagh,mcts_probs_batch_bagh,winner_batch_bagh,self.config.epochs)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "                newbaghplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                oldgoatplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "                print(\"Pitting new bagh model vs previous goat model\")\n",
        "                pwin, draw, nwin =self.start_play_batch_competitve(oldgoatplayer, newbaghplayer, n_games=10, show=False)\n",
        "                print([nwin, draw, pwin])\n",
        "                if (nwin-pwin>self.oldbagh):\n",
        "                    print(\"New bagh model accepted\")\n",
        "                    self.turn=\"G\"\n",
        "                    self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                    modelname='acceptedbagh_'+str(i)+'.h5'\n",
        "                    self.oldgoat=pwin-nwin\n",
        "                else:\n",
        "                    print(\"New bagh model rejected\")\n",
        "                    modelname='rejectedbagh_'+str(i)+'.h5'\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, modelname))\n",
        "            else:\n",
        "                self.turn=\"G\"\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'acceptedbagh_0.h5'))\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        for i in range(150):\n",
        "            try:\n",
        "                self.collect_compplay_data(self.config.play_batch_size)\n",
        "                self.policy_update_competitive(i)\n",
        "            except KeyboardInterrupt:\n",
        "                print('\\n\\rquit')\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        self.policy_value_net_goat.model.summary()\n",
        "        self.policy_value_net_bagh.model.summary()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TrainPipelineCompetitive().run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1fEyLCE4EoL"
      },
      "source": [
        "#Compeititve agents with human play data injection\n",
        "class TrainPipelineCompetitive():\n",
        "    def __init__(self, init_model=None):\n",
        "\n",
        "        # params of the board and the game\n",
        "        self.game = Game()\n",
        "        self.turn = \"G\"\n",
        "        # training params\n",
        "        self.config=TrainConfig()\n",
        "        self.greedy_config=TrainGreedyConfig()\n",
        "        self.play_config=PlayConfig()\n",
        "        self.data_buffer = deque(maxlen=self.config.buffer_size)\n",
        "        self.data_buffer_human  = deque(maxlen=self.config.buffer_size)\n",
        "        self.oldgoat=-1\n",
        "        self.oldbagh=-1\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net_goat = PolicyValueNet()\n",
        "            self.policy_value_net_bagh = PolicyValueNet()\n",
        "\n",
        "        self.mcts_player_goat = MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        self.mcts_player_bagh = MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "        # self.mcts_player_greedy = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "        #                               cpuct=self.greedy_config.c_puct,\n",
        "        #                               n_playout=self.greedy_config.n_playout,\n",
        "        #                               is_selfplay=1)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    def collect_humanplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Human-Play Data\")\n",
        "        print(\"Human-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            humplayer=HumanPlayer()\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.mcts_player_goat=MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.play_config.c_puct,\n",
        "                                      n_playout=self.play_config.n_playout)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.mcts_player_bagh=MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.play_config.c_puct,\n",
        "                                      n_playout=self.play_config.n_playout)\n",
        "            winner, play_data = self.game.start_play(humplayer, self.mcts_player_bagh)\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer_human.extend(play_data)\n",
        "\n",
        "            winner, play_data = self.game.start_play(self.mcts_player_goat, humplayer)\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer_human.extend(play_data)\n",
        "\n",
        "        print(\"\\n Human-Play Data Collection Completed\")\n",
        "\n",
        "\n",
        "    def collect_compplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        print(\"Collecting Comp-Play Data\")\n",
        "        print(\"Comp-Play Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            # winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "            #                                               temp=self.config.temp\n",
        "            #                                               ,greedy_player=self.mcts_player_greedy,who_greedy=\"B\")\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "                self.policy_value_net_goat=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                self.mcts_player_goat=MCTSPlayer(self.policy_value_net_goat.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "                self.policy_value_net_bagh=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                self.mcts_player_bagh=MCTSPlayer(self.policy_value_net_bagh.policy_value_fn,\n",
        "                                      cpuct=self.config.c_puct,\n",
        "                                      n_playout=self.config.n_playout)\n",
        "            winner, play_data = self.game.play_games(self.mcts_player_goat, self.mcts_player_bagh, show=False, temp=self.config.temp)\n",
        "            print(winner, end=\" \")\n",
        "            play_data = list(play_data)\n",
        "            # augment the data\n",
        "            play_data = symmetry_board_moves(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "        print(\"\\n Comp-Play Data Collection Completed\")\n",
        "\n",
        "    def start_play_batch_competitve (self, GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=self.game.play_games(GoatPlayer, BaghPlayer, show=False, temp=self.play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "\n",
        "    def policy_update_competitive(self, i):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        self.saveTrainExamples()\n",
        "        self.saveHumanExamples()\n",
        "        # if i%10==0 and i!=0:\n",
        "        if i%10==0:\n",
        "            self.saveHumanExamples()\n",
        "        \n",
        "        state_batch = [data[0] for data in self.data_buffer]\n",
        "        mcts_probs_batch = [data[1] for data in self.data_buffer]\n",
        "        winner_batch = [data[2] for data in self.data_buffer]\n",
        "        \n",
        "        # if i>=10:\n",
        "        if i>=0:\n",
        "            state_batch.extend([data[0] for data in self.data_buffer_human])\n",
        "            mcts_probs_batch.extend([data[1] for data in self.data_buffer_human])\n",
        "            winner_batch.extend([data[2] for data in self.data_buffer_human])\n",
        "        self.policy_value_net_goat.train(state_batch,mcts_probs_batch,winner_batch,self.config.epochs)\n",
        "        if os.path.exists(os.path.join(MODELPATH, 'best_goat.h5')):\n",
        "            self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "            newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_goat.h5'))\n",
        "            newgoatplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "            oldbaghplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            print(\"Pitting new goat model vs previous bagh model\")\n",
        "            nwin, draw, pwin =self.start_play_batch_competitve(newgoatplayer, oldbaghplayer, n_games=10, show=False)\n",
        "            print([nwin, draw, pwin])\n",
        "            if (nwin-pwin>self.oldgoat):\n",
        "                print(\"New goat model accepted\")\n",
        "                self.turn=\"B\"\n",
        "                self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "                modelname='acceptedgoat_'+str(i)+'.h5'\n",
        "                self.oldbagh=pwin-nwin\n",
        "            else:\n",
        "                print(\"New goat model rejected\")\n",
        "                modelname='rejectedgoat_'+str(i)+'.h5'\n",
        "            self.policy_value_net_goat.save_model(os.path.join(MODELPATH, modelname))\n",
        "        else:\n",
        "            self.turn=\"B\"\n",
        "            self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "            self.policy_value_net_goat.save_model(os.path.join(MODELPATH, 'acceptedgoat_0.h5'))\n",
        "        # winner_batch2 = [data*-1 for data in winner_batch]\n",
        "        self.policy_value_net_bagh.train(state_batch,mcts_probs_batch,winner_batch,self.config.epochs)\n",
        "        if os.path.exists(os.path.join(MODELPATH, 'best_bagh.h5')):\n",
        "            self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "            newnet=PolicyValueNet(os.path.join(MODELPATH, 'temp_bagh.h5'))\n",
        "            newbaghplayer=MCTSPlayer(newnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            oldnet=PolicyValueNet(os.path.join(MODELPATH, 'best_goat.h5'))\n",
        "            oldgoatplayer=MCTSPlayer(oldnet.policy_value_fn, cpuct=self.play_config.c_puct, n_playout=self.play_config.n_playout)\n",
        "            print(\"Pitting new bagh model vs previous goat model\")\n",
        "            pwin, draw, nwin =self.start_play_batch_competitve(oldgoatplayer, newbaghplayer, n_games=10, show=False)\n",
        "            print([nwin, draw, pwin])\n",
        "            if (nwin-pwin>self.oldbagh):\n",
        "                print(\"New bagh model accepted\")\n",
        "                self.turn=\"G\"\n",
        "                self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "                modelname='acceptedbagh_'+str(i)+'.h5'\n",
        "                self.oldgoat=pwin-nwin\n",
        "            else:\n",
        "                print(\"New bagh model rejected\")\n",
        "                modelname='rejectedbagh_'+str(i)+'.h5'\n",
        "            self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, modelname))\n",
        "        else:\n",
        "            self.turn=\"G\"\n",
        "            self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'best_bagh.h5'))\n",
        "            self.policy_value_net_bagh.save_model(os.path.join(MODELPATH, 'acceptedbagh_0.h5'))\n",
        "\n",
        "    def saveTrainExamples(self):\n",
        "        filename = os.path.join(MODELPATH, \"complaydata.examples\")\n",
        "        with open(filename, \"wb+\") as f:\n",
        "            Pickler(f).dump(self.data_buffer)\n",
        "        f.closed\n",
        "        \n",
        "\n",
        "    def saveHumanExamples(self):\n",
        "        filename = os.path.join(MODELPATH, \"humplaydata.examples\")\n",
        "        with open(filename, \"wb+\") as f:\n",
        "            Pickler(f).dump(self.data_buffer_human)\n",
        "        f.closed\n",
        "\n",
        "    def loadTrainExamples(self):\n",
        "        examplesFile = os.path.join(MODELPATH, \"complaydata.examples\")\n",
        "        if os.path.isfile(examplesFile):\n",
        "            with open(examplesFile, \"rb\") as f:\n",
        "                self.data_buffer = Unpickler(f).load()\n",
        "\n",
        "    def loadHumanExamples(self):\n",
        "        examplesFile = os.path.join(MODELPATH, \"humplaydata.examples\")\n",
        "        if os.path.isfile(examplesFile):\n",
        "            with open(examplesFile, \"rb\") as f:\n",
        "                self.data_buffer_human = Unpickler(f).load()\n",
        "            \n",
        "    \n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        self.loadTrainExamples()\n",
        "        self.loadHumanExamples()\n",
        "\n",
        "        # if os.path.exists(os.path.join(MODELPATH, 'humanplaydata')):\n",
        "        #     file = open(os.path.join(MODELPATH, 'humanplaydata'))\n",
        "        #     csvreader = csv.reader(file)\n",
        "        #     rows = []\n",
        "        #     for row in csvreader:\n",
        "        #         rows.append(row)\n",
        "        #     file.close()\n",
        "        #     self.data_buffer_human=rows[0]\n",
        "\n",
        "        for i in range(150):\n",
        "            try:\n",
        "                # if i%10==0 and i!=0:\n",
        "                if i%10==0:\n",
        "                    self.collect_humanplay_data(2)\n",
        "                self.collect_compplay_data(self.config.play_batch_size)\n",
        "                self.policy_update_competitive(i+21)\n",
        "            except KeyboardInterrupt:\n",
        "                print('\\n\\rquit')\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        self.policy_value_net_goat.model.summary()\n",
        "        self.policy_value_net_bagh.model.summary()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TrainPipelineCompetitive().run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3CdGa_gRuNS"
      },
      "source": [
        "#Pitting Self play agents with first accepted agents\n",
        "game = Game()\n",
        "play_config=PlayConfig()\n",
        "# all players\n",
        "basenet=PolicyValueNet(os.path.join(MODELPATH, 'accepted_0.h5'))\n",
        "baseplayer=MCTSPlayer(basenet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "nwinlist=[]\n",
        "pwinlist=[]\n",
        "drawlist=[]\n",
        "adjustedwin=[]\n",
        "def start_play_batch (GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=game.play_games(GoatPlayer, BaghPlayer, show=False, temp=play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "for i in range(100):\n",
        "    filename='accepted_'+str(i)+'.h5'\n",
        "    print(filename)\n",
        "    if os.path.exists(os.path.join(MODELPATH, filename)):\n",
        "        acceptednet=PolicyValueNet(os.path.join(MODELPATH, filename))\n",
        "        acceptedplayer=MCTSPlayer(acceptednet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "        player2 = baseplayer  # Player 2 is neural network if it's cpu vs cpu.\n",
        "\n",
        "        print(\"Accepted player as goat\")\n",
        "        nwing, drawg, pwing =start_play_batch(acceptedplayer, baseplayer , n_games=10, show=False)\n",
        "        print([nwing, drawg, pwing])\n",
        "        print(\"Accepted player as bagh\")\n",
        "        pwinb, drawb, nwinb =start_play_batch(baseplayer, acceptedplayer, n_games=10, show=False)\n",
        "        print([nwinb, drawb, pwinb])\n",
        "        print(\"Pitting Completed\")\n",
        "        nwin=nwing+nwinb\n",
        "        pwin=pwing+pwinb\n",
        "        draw=drawg+drawb\n",
        "        nwinlist.append(nwin/20)\n",
        "        pwinlist.append(pwin/20)\n",
        "        drawlist.append(draw/20)\n",
        "        adjustedwin.append((nwin)/(20-draw)\n",
        "        print([nwin, draw, pwin])\n",
        "        \n",
        "        f = open(os.path.join(RESULTPATH, 'selfresults'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwin, draw, pwin]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "    else:\n",
        "        print('file not found')\n",
        "\n",
        "plt.plot(range(1,len(nwinlist)+1),nwinlist, label='Accepted agent win probability')\n",
        "plt.plot(range(1,len(pwinlist)+1),pwinlist, label='Accepted agent loss probability')\n",
        "plt.plot(range(1,len(drawlist)+1),drawlist, label='Accepted agent draw probability')\n",
        "plt.legend()\n",
        "plt.title('Accepted agents vs the first accepted agent')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Accepted Agents')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1,len(adjustedwin)+1),adjustedwin, label='Accepted agent adjusted win probability')\n",
        "plt.legend()\n",
        "plt.title('Accepted agents vs the first accepted agent (adjusted win rate)')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Accepted Agents')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLxhg1DrGwUA"
      },
      "source": [
        "#Pitting AT agents with self_play agents\n",
        "game = Game()\n",
        "play_config=PlayConfig()\n",
        "# all players\n",
        "bestagents=['accepted_0.h5',\n",
        "            'accepted_61.h5',\n",
        "            'accepted_58.h5',\n",
        "            'accepted_55.h5',\n",
        "            'accepted_45.h5',\n",
        "            'accepted_32.h5']\n",
        "            \n",
        "goatnet=PolicyValueNet(os.path.join(COMPPATH, 'best_goat.h5'))\n",
        "goatplayer=MCTSPlayer(goatnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "baghnet=PolicyValueNet(os.path.join(COMPPATH, 'best_bagh.h5'))\n",
        "baghplayer=MCTSPlayer(baghnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "def start_play_batch (GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=game.play_games(GoatPlayer, BaghPlayer, show=False, temp=play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "for agent in bestagents:\n",
        "    print(agent)\n",
        "    if os.path.exists(os.path.join(MODELPATH, agent)):\n",
        "        acceptednet=PolicyValueNet(os.path.join(MODELPATH, agent))\n",
        "        acceptedplayer=MCTSPlayer(acceptednet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "        print(\"Goat Player\")\n",
        "        nwing, drawg, pwing =start_play_batch(goatplayer, acceptedplayer , n_games=20, show=False)\n",
        "        print([nwing, drawg, pwing])\n",
        "\n",
        "        f = open(os.path.join(RESULTPATH, 'compresultsgoat'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwing, drawg, pwing]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "\n",
        "        print(\"Bagh Player\")\n",
        "        pwinb, drawb, nwinb =start_play_batch(acceptedplayer, baghplayer, n_games=20, show=False)\n",
        "        print([nwinb, drawb, pwinb])\n",
        "        \n",
        "        f = open(os.path.join(RESULTPATH, 'compresultsbagh'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwinb, drawb, pwinb]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "    else:\n",
        "        print('file not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS0nhclMzq4K"
      },
      "source": [
        "#Pitting Acceptance condition changed agents with self play agents\n",
        "game = Game()\n",
        "play_config=PlayConfig()\n",
        "# all players\n",
        "bestagents=['accepted_0.h5',\n",
        "            'accepted_61.h5',\n",
        "            'accepted_58.h5',\n",
        "            'accepted_55.h5',\n",
        "            'accepted_45.h5',\n",
        "            'accepted_32.h5']\n",
        "            \n",
        "goatnet=PolicyValueNet(os.path.join(ACCEPTPATH, 'best_goat.h5'))\n",
        "goatplayer=MCTSPlayer(goatnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "baghnet=PolicyValueNet(os.path.join(ACCEPTPATH, 'best_bagh.h5'))\n",
        "baghplayer=MCTSPlayer(baghnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "def start_play_batch (GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=game.play_games(GoatPlayer, BaghPlayer, show=False, temp=play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "for agent in bestagents:\n",
        "    print(agent)\n",
        "    if os.path.exists(os.path.join(MODELPATH, agent)):\n",
        "        acceptednet=PolicyValueNet(os.path.join(MODELPATH, agent))\n",
        "        acceptedplayer=MCTSPlayer(acceptednet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "        print(\"Goat Player\")\n",
        "        nwing, drawg, pwing =start_play_batch(goatplayer, acceptedplayer , n_games=20, show=False)\n",
        "        print([nwing, drawg, pwing])\n",
        "\n",
        "        f = open(os.path.join(RESULTPATH, 'acceptresultsgoat'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwing, drawg, pwing]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "\n",
        "        print(\"Bagh Player\")\n",
        "        pwinb, drawb, nwinb =start_play_batch(acceptedplayer, baghplayer, n_games=20, show=False)\n",
        "        print([nwinb, drawb, pwinb])\n",
        "        \n",
        "        f = open(os.path.join(RESULTPATH, 'acceptresultsbagh'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwinb, drawb, pwinb]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "    else:\n",
        "        print('file not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOsYEIUxHp7N"
      },
      "source": [
        "#Pitting Data filter agents with self play agents\n",
        "game = Game()\n",
        "play_config=PlayConfig()\n",
        "# all players\n",
        "bestagents=['accepted_0.h5',\n",
        "            'accepted_61.h5',\n",
        "            'accepted_58.h5',\n",
        "            'accepted_55.h5',\n",
        "            'accepted_45.h5',\n",
        "            'accepted_32.h5']\n",
        "            \n",
        "goatnet=PolicyValueNet(os.path.join(FILTERPATH, 'best_goat.h5'))\n",
        "goatplayer=MCTSPlayer(goatnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "baghnet=PolicyValueNet(os.path.join(FILTERPATH, 'best_bagh.h5'))\n",
        "baghplayer=MCTSPlayer(baghnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "def start_play_batch (GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=game.play_games(GoatPlayer, BaghPlayer, show=False, temp=play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "for agent in bestagents:\n",
        "    print(agent)\n",
        "    if os.path.exists(os.path.join(MODELPATH, agent)):\n",
        "        acceptednet=PolicyValueNet(os.path.join(MODELPATH, agent))\n",
        "        acceptedplayer=MCTSPlayer(acceptednet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "        print(\"Goat Player\")\n",
        "        nwing, drawg, pwing =start_play_batch(goatplayer, acceptedplayer , n_games=20, show=False)\n",
        "        print([nwing, drawg, pwing])\n",
        "\n",
        "        f = open(os.path.join(RESULTPATH, 'filterresultsgoat'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwing, drawg, pwing]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "\n",
        "        print(\"Bagh Player\")\n",
        "        pwinb, drawb, nwinb =start_play_batch(acceptedplayer, baghplayer, n_games=20, show=False)\n",
        "        print([nwinb, drawb, pwinb])\n",
        "        \n",
        "        f = open(os.path.join(RESULTPATH, 'filterresultsbagh'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwinb, drawb, pwinb]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "    else:\n",
        "        print('file not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rqJDZ-QH30K"
      },
      "source": [
        "#Pitting human play agents with self play agents\n",
        "game = Game()\n",
        "play_config=PlayConfig()\n",
        "# all players\n",
        "bestagents=['accepted_0.h5',\n",
        "            'accepted_61.h5',\n",
        "            'accepted_58.h5',\n",
        "            'accepted_55.h5',\n",
        "            'accepted_45.h5',\n",
        "            'accepted_32.h5']\n",
        "            \n",
        "goatnet=PolicyValueNet(os.path.join(HUMANPATH, 'best_goat_19.h5'))\n",
        "goatplayer=MCTSPlayer(goatnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "baghnet=PolicyValueNet(os.path.join(HUMANPATH, 'best_bagh.h5'))\n",
        "baghplayer=MCTSPlayer(baghnet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "def start_play_batch (GoatPlayer, BaghPlayer, n_games=1, show=True):\n",
        "        goat_count=0\n",
        "        draw_count=0\n",
        "        bagh_count=0\n",
        "        print(\"Pitting Games\", end =\" \")\n",
        "        for i in range(n_games):\n",
        "            print(str(i), end =\" \")\n",
        "            winner, data=game.play_games(GoatPlayer, BaghPlayer, show=False, temp=play_config.temp)\n",
        "            print(winner, end =\" \")\n",
        "            if (winner==\"B\"):\n",
        "                bagh_count+=1\n",
        "            elif (winner==\"G\"):\n",
        "                goat_count+=1\n",
        "            else:\n",
        "                draw_count+=1\n",
        "        return goat_count, draw_count, bagh_count\n",
        "\n",
        "for agent in bestagents:\n",
        "    print(agent)\n",
        "    if os.path.exists(os.path.join(MODELPATH, agent)):\n",
        "        acceptednet=PolicyValueNet(os.path.join(MODELPATH, agent))\n",
        "        acceptedplayer=MCTSPlayer(acceptednet.policy_value_fn, cpuct=play_config.c_puct, n_playout=play_config.n_playout)\n",
        "\n",
        "        print(\"Goat Player\")\n",
        "        nwing, drawg, pwing =start_play_batch(goatplayer, acceptedplayer , n_games=20, show=False)\n",
        "        print([nwing, drawg, pwing])\n",
        "\n",
        "        f = open(os.path.join(RESULTPATH, 'humanresultsgoat'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwing, drawg, pwing]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "\n",
        "        print(\"Bagh Player\")\n",
        "        pwinb, drawb, nwinb =start_play_batch(acceptedplayer, baghplayer, n_games=20, show=False)\n",
        "        print([nwinb, drawb, pwinb])\n",
        "        \n",
        "        f = open(os.path.join(RESULTPATH, 'humanresultsbagh'), 'a')\n",
        "        writer = csv.writer(f)\n",
        "        tempscore=[nwinb, drawb, pwinb]\n",
        "        writer.writerow(tempscore)\n",
        "        f.close()\n",
        "    else:\n",
        "        print('file not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8wX-lcN4DMX"
      },
      "source": [
        "#Generating graphs from csv\n",
        "file = open(os.path.join(RESULTPATH, 'selfresults'))\n",
        "csvreader = csv.reader(file)\n",
        "rows = []\n",
        "for row in csvreader:\n",
        "    rows.append(row)\n",
        "file.close()\n",
        "\n",
        "file2 = open(os.path.join(RESULTPATH, 'compresultsgoat'))\n",
        "csvreader2 = csv.reader(file2)\n",
        "rows2 = []\n",
        "for row in csvreader2:\n",
        "    rows2.append(row)\n",
        "file2.close()\n",
        "\n",
        "file3 = open(os.path.join(RESULTPATH, 'compresultsbagh'))\n",
        "csvreader3 = csv.reader(file3)\n",
        "rows3 = []\n",
        "for row in csvreader3:\n",
        "    rows3.append(row)\n",
        "file3.close()\n",
        "\n",
        "file4 = open(os.path.join(RESULTPATH, 'acceptresultsgoat'))\n",
        "csvreader4 = csv.reader(file4)\n",
        "rows4 = []\n",
        "for row in csvreader4:\n",
        "    rows4.append(row)\n",
        "file4.close()\n",
        "\n",
        "file5 = open(os.path.join(RESULTPATH, 'acceptresultsbagh'))\n",
        "csvreader5 = csv.reader(file5)\n",
        "rows5 = []\n",
        "for row in csvreader5:\n",
        "    rows5.append(row)\n",
        "file5.close()\n",
        "\n",
        "file6 = open(os.path.join(RESULTPATH, 'filterresultsgoat'))\n",
        "csvreader6 = csv.reader(file6)\n",
        "rows6 = []\n",
        "for row in csvreader6:\n",
        "    rows6.append(row)\n",
        "file6.close()\n",
        "\n",
        "file7 = open(os.path.join(RESULTPATH, 'filterresultsbagh'))\n",
        "csvreader7 = csv.reader(file7)\n",
        "rows7 = []\n",
        "for row in csvreader7:\n",
        "    rows7.append(row)\n",
        "file7.close()\n",
        "\n",
        "file8 = open(os.path.join(RESULTPATH, 'humanresultsgoat'))\n",
        "csvreader8 = csv.reader(file8)\n",
        "rows8 = []\n",
        "for row in csvreader8:\n",
        "    rows8.append(row)\n",
        "file8.close()\n",
        "\n",
        "file9 = open(os.path.join(RESULTPATH, 'humanresultsbagh'))\n",
        "csvreader9 = csv.reader(file9)\n",
        "rows9 = []\n",
        "for row in csvreader9:\n",
        "    rows9.append(row)\n",
        "file9.close()\n",
        "\n",
        "nwin=[int(row[0]) for row in rows]\n",
        "draw=[int(row[1]) for row in rows]\n",
        "pwin=[int(row[2]) for row in rows]\n",
        "nwinlist=[int(row[0])/20 for row in rows]\n",
        "pwinlist=[int(row[2])/20 for row in rows]\n",
        "drawlist=[int(row[1])/20 for row in rows]\n",
        "\n",
        "nwincompgoat=[int(row[0]) for row in rows2]\n",
        "drawcompgoat=[int(row[1]) for row in rows2]\n",
        "pwincompgoat=[int(row[2]) for row in rows2]\n",
        "nwinlistcompgoat=[int(row[0])/20 for row in rows2]\n",
        "pwinlistcompgoat=[int(row[2])/20 for row in rows2]\n",
        "drawlistcompgoat=[int(row[1])/20 for row in rows2]\n",
        "\n",
        "nwincompbagh=[int(row[0]) for row in rows3]\n",
        "drawcompbagh=[int(row[1]) for row in rows3]\n",
        "pwincompbagh=[int(row[2]) for row in rows3]\n",
        "nwinlistcompbagh=[int(row[0])/20 for row in rows3]\n",
        "pwinlistcompbagh=[int(row[2])/20 for row in rows3]\n",
        "drawlistcompbagh=[int(row[1])/20 for row in rows3]\n",
        "\n",
        "\n",
        "nwinlistacceptgoat=[int(row[0])/20 for row in rows4]\n",
        "pwinlistacceptgoat=[int(row[2])/20 for row in rows4]\n",
        "drawlistacceptgoat=[int(row[1])/20 for row in rows4]\n",
        "\n",
        "nwinlistacceptbagh=[int(row[0])/20 for row in rows5]\n",
        "pwinlistacceptbagh=[int(row[2])/20 for row in rows5]\n",
        "drawlistacceptbagh=[int(row[1])/20 for row in rows5]\n",
        "\n",
        "nwinlistfiltergoat=[int(row[0])/20 for row in rows6]\n",
        "pwinlistfiltergoat=[int(row[2])/20 for row in rows6]\n",
        "drawlistfiltergoat=[int(row[1])/20 for row in rows6]\n",
        "\n",
        "nwinlistfilterbagh=[int(row[0])/20 for row in rows7]\n",
        "pwinlistfilterbagh=[int(row[2])/20 for row in rows7]\n",
        "drawlistfilterbagh=[int(row[1])/20 for row in rows7]\n",
        "\n",
        "nwinlisthumangoat=[int(row[0])/20 for row in rows8]\n",
        "pwinlisthumangoat=[int(row[2])/20 for row in rows8]\n",
        "drawlisthumangoat=[int(row[1])/20 for row in rows8]\n",
        "\n",
        "nwinlisthumanbagh=[int(row[0])/20 for row in rows9]\n",
        "pwinlisthumanbagh=[int(row[2])/20 for row in rows9]\n",
        "drawlisthumanbagh=[int(row[1])/20 for row in rows9]\n",
        "\n",
        "adjustedwin=[(int(row[0]))/(20-int(row[2])) for row in rows]\n",
        "\n",
        "#graphs for self play agents\n",
        "plt.plot(range(0,len(nwinlist)),nwinlist, label='Accepted agent win rate')\n",
        "plt.plot(range(0,len(pwinlist)),pwinlist, label='Accepted agent lose rate')\n",
        "plt.plot(range(0,len(drawlist)),drawlist, label='Accepted agent draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accepted agents vs the first accepted agent (base)')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('Accepted self play agents')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(0,len(nwin)),nwin, label='Accepted agent wins')\n",
        "plt.plot(range(0,len(pwin)),pwin, label='Accepted agent losses')\n",
        "plt.plot(range(0,len(pwin)),draw, label='Accepted agent draws')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accepted agents vs the first accepted agent (count)')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Accepted Agents')\n",
        "plt.show()\n",
        "\n",
        "#graphs for alternate training agents\n",
        "plt.plot(range(0,len(nwinlistcompgoat)),nwinlistcompgoat, label='Comp Goat Win rate')\n",
        "plt.plot(range(0,len(pwinlistcompgoat)),pwinlistcompgoat, label='Comp Goat lose rate')\n",
        "plt.plot(range(0,len(drawlistcompgoat)),drawlistcompgoat, label='Comp Goat draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best AT goat vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(0,len(nwinlistcompbagh)),nwinlistcompbagh, label='Comp Bagh win rate')\n",
        "plt.plot(range(0,len(pwinlistcompbagh)),pwinlistcompbagh, label='Comp Bagh lose rate')\n",
        "plt.plot(range(0,len(drawlistcompbagh)),drawlistcompbagh, label='Comp Bagh draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best AT tiger vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "#graphs for accepted condition changed agents\n",
        "plt.plot(range(0,len(nwinlistacceptgoat)),nwinlistacceptgoat, label='Comp Goat Win rate')\n",
        "plt.plot(range(0,len(pwinlistacceptgoat)),pwinlistacceptgoat, label='Comp Goat lose rate')\n",
        "plt.plot(range(0,len(drawlistacceptgoat)),drawlistacceptgoat, label='Comp Goat draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best ACC goat vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(0,len(nwinlistacceptbagh)),nwinlistacceptbagh, label='Comp Bagh win rate')\n",
        "plt.plot(range(0,len(pwinlistacceptbagh)),pwinlistacceptbagh, label='Comp Bagh lose rate')\n",
        "plt.plot(range(0,len(drawlistacceptbagh)),drawlistacceptbagh, label='Comp Bagh draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best ACC tiger vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "#graphs for data filter agents\n",
        "plt.plot(range(0,len(nwinlistfiltergoat)),nwinlistfiltergoat, label='Comp Goat Win rate')\n",
        "plt.plot(range(0,len(pwinlistfiltergoat)),pwinlistfiltergoat, label='Comp Goat lose rate')\n",
        "plt.plot(range(0,len(drawlistfiltergoat)),drawlistfiltergoat, label='Comp Goat draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best DF goat vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(0,len(nwinlistfilterbagh)),nwinlistfilterbagh, label='Comp Bagh win rate')\n",
        "plt.plot(range(0,len(pwinlistfilterbagh)),pwinlistfilterbagh, label='Comp Bagh lose rate')\n",
        "plt.plot(range(0,len(drawlistfilterbagh)),drawlistfilterbagh, label='Comp Bagh draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best DF tiger vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "#graphs for human play agents\n",
        "plt.plot(range(0,len(nwinlisthumangoat)),nwinlisthumangoat, label='Comp Goat Win rate')\n",
        "plt.plot(range(0,len(pwinlisthumangoat)),pwinlisthumangoat, label='Comp Goat lose rate')\n",
        "plt.plot(range(0,len(drawlisthumangoat)),drawlisthumangoat, label='Comp Goat draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best HPI goat vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(0,len(nwinlisthumanbagh)),nwinlisthumanbagh, label='Comp Bagh win rate')\n",
        "plt.plot(range(0,len(pwinlisthumanbagh)),pwinlisthumanbagh, label='Comp Bagh lose rate')\n",
        "plt.plot(range(0,len(drawlisthumanbagh)),drawlisthumanbagh, label='Comp Bagh draw rate')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Best HPI tiger vs base(0) & best five (1-5) self-play agents')\n",
        "plt.ylabel('Probability')\n",
        "plt.xlabel('1st accepted followed by best 5 accepted agent')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-2ilcnHDQVd"
      },
      "source": [
        "#Play a game against the agent\n",
        "game=Game()\n",
        "baghnet=PolicyValueNet(os.path.join(FILTERPATH, 'acceptedbagh_0.h5'))\n",
        "goatnet=PolicyValueNet(os.path.join(HUMANPATH, 'best_goat.h5'))\n",
        "goat=MCTSPlayer(goatnet.policy_value_fn, cpuct=5, n_playout=20)\n",
        "bagh=MCTSPlayer(baghnet.policy_value_fn, cpuct=5, n_playout=20)\n",
        "# player=HumanPlayer()\n",
        "\n",
        "#change the players here to play as goat and tiger\n",
        "data=game.start_play(goat,bagh)\n",
        "\n",
        "# if you want to train further with human games\n",
        "# data=[x for x in data]\n",
        "# data=symmetry_board_moves(data)\n",
        "# state_batch = [x[0] for x in data]\n",
        "# mcts_probs_batch = [x[1] for x in data]\n",
        "# winner_batch = [x[2] for x in data]\n",
        "# pvnet.train(state_batch,mcts_probs_batch,winner_batch,5)\n",
        "# pvnet.save_model(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}